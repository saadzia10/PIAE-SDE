{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d9c3aed-648a-4759-a55f-383af65e6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f7266-5e08-4dc5-82fd-e37c1d279e27",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b48af8-c3e4-4f22-b6cd-1600fcc65a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"../phys-vae/data/climate/\")\n",
    "data = pd.read_parquet(\"complete_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a49d8-aad0-4e83-b2b8-79c80df19872",
   "metadata": {},
   "source": [
    "### Cleaning and NaN removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f753a17-435c-48b3-ba45-dfb8952d73d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gap Length</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>815 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gap Length  Frequency\n",
       "0             1       8619\n",
       "1             2       4881\n",
       "2             3       3767\n",
       "3             4       3013\n",
       "4             5       2540\n",
       "..          ...        ...\n",
       "810         751          1\n",
       "811         749          1\n",
       "812         748          1\n",
       "813         747          1\n",
       "814         815          1\n",
       "\n",
       "[815 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace -9999 with NaN\n",
    "data['NEE'] = data['NEE'].replace(-9999, np.nan)\n",
    "\n",
    "# Identify the consecutive gaps\n",
    "data['Gap'] = data['NEE'].isna().astype(int).groupby(data['NEE'].notna().astype(int).cumsum()).cumsum()\n",
    "\n",
    "# Filter out the zero values and calculate the lengths of the gaps\n",
    "gap_lengths = data[data['Gap'] != 0]['Gap'].value_counts().reset_index()\n",
    "gap_lengths.columns = ['Gap Length', 'Frequency']\n",
    "\n",
    "gap_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105a9519-5637-49b7-a2d4-9ca894d57234",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df630ed4-8edc-4e90-8f47-a0990dae9c67",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdcb6cc-b61b-402e-a533-33606b0e9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_stats(df, flux, scale=\"30T\"):\n",
    "    df = df.copy()\n",
    "    ind = df.index\n",
    "    dt = df[\"DateTime\"]\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "    flux_orig = df[flux].copy()\n",
    "    df[flux] = df[flux].interpolate().values\n",
    "    \n",
    "    # Set max\n",
    "    flux_max = df[flux].resample(\"D\").max()\n",
    "    df[\"flux_max\"] = flux_max.resample(scale).bfill()\n",
    "    \n",
    "    # Set min\n",
    "    flux_min = df[flux].resample(\"D\").min()\n",
    "    df[\"flux_min\"] = flux_min.resample(scale).bfill()\n",
    "    \n",
    "    # Set mean\n",
    "    flux_mean = df[flux].resample(\"D\").mean()\n",
    "    df[\"flux_mean\"] = flux_mean.resample(scale).bfill()\n",
    "    \n",
    "    # Set std\n",
    "    flux_std = df[flux].resample(\"D\").std()\n",
    "    df[\"flux_std\"] = flux_std.resample(scale).bfill()\n",
    "    \n",
    "    # Set 25%, 50%, 75% quantiles\n",
    "    flux_p25 = df[flux].resample(\"D\").quantile(0.25)\n",
    "    df[\"flux_p25\"] = flux_p25.resample(scale).bfill()\n",
    "    \n",
    "    flux_p50 = df[flux].resample(\"D\").quantile(0.50)\n",
    "    df[\"flux_p50\"] = flux_p50.resample(scale).bfill()\n",
    "    \n",
    "    flux_p75 = df[flux].resample(\"D\").quantile(0.75)\n",
    "    df[\"flux_p75\"] = flux_p75.resample(scale).bfill()\n",
    "    \n",
    "    df = df.interpolate()\n",
    "    \n",
    "    df.index = ind\n",
    "    df[\"DateTime\"] = dt.values\n",
    "    df.loc[:, flux] = flux_orig.values\n",
    "    return df, [\"flux_max\", \"flux_min\", \"flux_mean\", \"flux_std\", \"flux_p25\", \"flux_p50\", \"flux_p75\"]\n",
    " \n",
    "#====================================================================================================\n",
    "# set season tag\n",
    "def set_season_tag(df, isnorth = True):\n",
    "    if isnorth:\n",
    "        df[\"season\"] = (df['DateTime'].month%12 + 3) // 3 # print(seasons)\n",
    "    else:\n",
    "        df[\"season\"] = ((df['DateTime'].month + 6)%12 + 3)//3\n",
    "    return df, [\"season\"]\n",
    "#====================================================================================================\n",
    "# set radiance tag\n",
    "\n",
    "def set_rg_tag(df, rg):\n",
    "    df[\"rg_rank\"] = np.select(\n",
    "        condlist = [\n",
    "            df[rg] < 10,\n",
    "            (df[rg] > 10) & (df[rg] < 100),\n",
    "            df[rg] > 100\n",
    "        ],\n",
    "        choicelist = [\n",
    "            1,\n",
    "            2,\n",
    "            3\n",
    "        ],\n",
    "        default = 0\n",
    "    )\n",
    "    return df, [\"rg_rank\"]\n",
    "\n",
    "def extract_features(data):\n",
    "    \"\"\"\n",
    "    Extract features from the timestamp for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame with 'Timestamp' and 'NEE' columns.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with extracted features.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['hour'] = data['DateTime'].dt.hour\n",
    "    data['dayofweek'] = data['DateTime'].dt.dayofweek\n",
    "    data['month'] = data['DateTime'].dt.month\n",
    "    data['dayofyear'] = data['DateTime'].dt.dayofyear\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9310afbd-4724-49bb-9d22-7988c26ef67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, stat_tags = set_stats(data, \"NEE\")\n",
    "data, rg_tag = set_rg_tag(data, 'Rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc35a432-22b4-45a3-abb6-3b0ba6f1bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, stat_tags = set_stats(data, \"NEE\")\n",
    "data, rg_tag = set_rg_tag(data, 'Rg')\n",
    "data = extract_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a4bd6a-6b13-4ab6-85af-f17bc09ac8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEE</th>\n",
       "      <th>NEE_unc</th>\n",
       "      <th>LE</th>\n",
       "      <th>LE_unc</th>\n",
       "      <th>H</th>\n",
       "      <th>H_unc</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Tau_unc</th>\n",
       "      <th>CO2_strg</th>\n",
       "      <th>LE_strg</th>\n",
       "      <th>...</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_p25</th>\n",
       "      <th>flux_p50</th>\n",
       "      <th>flux_p75</th>\n",
       "      <th>rg_rank</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.5670</td>\n",
       "      <td>3.06210</td>\n",
       "      <td>-9999.000</td>\n",
       "      <td>14.6310</td>\n",
       "      <td>-2.9117</td>\n",
       "      <td>0.72031</td>\n",
       "      <td>0.17393</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>-0.103920</td>\n",
       "      <td>-0.512280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 00:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7892</td>\n",
       "      <td>2.06360</td>\n",
       "      <td>37.362</td>\n",
       "      <td>12.3280</td>\n",
       "      <td>-10.5920</td>\n",
       "      <td>0.64084</td>\n",
       "      <td>0.17662</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.089560</td>\n",
       "      <td>-0.189890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0302</td>\n",
       "      <td>0.54104</td>\n",
       "      <td>35.169</td>\n",
       "      <td>4.7136</td>\n",
       "      <td>-15.9840</td>\n",
       "      <td>1.20660</td>\n",
       "      <td>0.18260</td>\n",
       "      <td>0.013019</td>\n",
       "      <td>-0.204920</td>\n",
       "      <td>-0.404700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 01:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0909</td>\n",
       "      <td>0.33387</td>\n",
       "      <td>32.364</td>\n",
       "      <td>3.5567</td>\n",
       "      <td>-13.9460</td>\n",
       "      <td>0.63822</td>\n",
       "      <td>0.12357</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>-0.035944</td>\n",
       "      <td>-0.124000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8500</td>\n",
       "      <td>0.44881</td>\n",
       "      <td>28.569</td>\n",
       "      <td>3.8587</td>\n",
       "      <td>-14.0660</td>\n",
       "      <td>0.77249</td>\n",
       "      <td>0.15957</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 02:30:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.1455</td>\n",
       "      <td>0.98754</td>\n",
       "      <td>24.648</td>\n",
       "      <td>12.6020</td>\n",
       "      <td>-14.1820</td>\n",
       "      <td>0.76101</td>\n",
       "      <td>0.13249</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>0.143340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.4408</td>\n",
       "      <td>0.80687</td>\n",
       "      <td>11.048</td>\n",
       "      <td>8.1544</td>\n",
       "      <td>-14.9320</td>\n",
       "      <td>0.80979</td>\n",
       "      <td>0.13693</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.201620</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-22 03:30:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.4772</td>\n",
       "      <td>0.80788</td>\n",
       "      <td>36.742</td>\n",
       "      <td>8.4677</td>\n",
       "      <td>-15.0720</td>\n",
       "      <td>1.20000</td>\n",
       "      <td>0.13514</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.022391</td>\n",
       "      <td>0.281180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-22 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.4707</td>\n",
       "      <td>0.66564</td>\n",
       "      <td>13.321</td>\n",
       "      <td>5.3211</td>\n",
       "      <td>-12.6100</td>\n",
       "      <td>0.75674</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.147020</td>\n",
       "      <td>0.222110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-22 04:30:00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.8830</td>\n",
       "      <td>0.53958</td>\n",
       "      <td>14.134</td>\n",
       "      <td>3.1002</td>\n",
       "      <td>-11.6310</td>\n",
       "      <td>1.00730</td>\n",
       "      <td>0.11883</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>-0.068287</td>\n",
       "      <td>0.151030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986102</td>\n",
       "      <td>4.82125</td>\n",
       "      <td>5.3522</td>\n",
       "      <td>5.86985</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-22 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NEE  NEE_unc        LE   LE_unc        H    H_unc      Tau   Tau_unc  \\\n",
       "0  12.5670  3.06210 -9999.000  14.6310  -2.9117  0.72031  0.17393  0.012095   \n",
       "1   6.7892  2.06360    37.362  12.3280 -10.5920  0.64084  0.17662  0.012124   \n",
       "2   4.0302  0.54104    35.169   4.7136 -15.9840  1.20660  0.18260  0.013019   \n",
       "3   5.0909  0.33387    32.364   3.5567 -13.9460  0.63822  0.12357  0.008703   \n",
       "4   5.8500  0.44881    28.569   3.8587 -14.0660  0.77249  0.15957  0.011172   \n",
       "5   4.1455  0.98754    24.648  12.6020 -14.1820  0.76101  0.13249  0.009131   \n",
       "6   3.4408  0.80687    11.048   8.1544 -14.9320  0.80979  0.13693  0.009080   \n",
       "7   3.4772  0.80788    36.742   8.4677 -15.0720  1.20000  0.13514  0.008611   \n",
       "8   5.4707  0.66564    13.321   5.3211 -12.6100  0.75674  0.14830  0.009997   \n",
       "9   4.8830  0.53958    14.134   3.1002 -11.6310  1.00730  0.11883  0.007229   \n",
       "\n",
       "   CO2_strg   LE_strg  ...  flux_std  flux_p25  flux_p50  flux_p75  rg_rank  \\\n",
       "0 -0.103920 -0.512280  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "1  0.089560 -0.189890  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "2 -0.204920 -0.404700  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "3 -0.035944 -0.124000  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "4  0.074542  0.005818  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "5  0.025845  0.143340  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "6  0.201620  0.302600  ...  0.986102   4.82125    5.3522   5.86985        1   \n",
       "7 -0.022391  0.281180  ...  0.986102   4.82125    5.3522   5.86985        2   \n",
       "8  0.147020  0.222110  ...  0.986102   4.82125    5.3522   5.86985        2   \n",
       "9 -0.068287  0.151030  ...  0.986102   4.82125    5.3522   5.86985        2   \n",
       "\n",
       "             DateTime  hour  dayofweek  month  dayofyear  \n",
       "0 2012-06-22 00:30:00     0          4      6        174  \n",
       "1 2012-06-22 01:00:00     1          4      6        174  \n",
       "2 2012-06-22 01:30:00     1          4      6        174  \n",
       "3 2012-06-22 02:00:00     2          4      6        174  \n",
       "4 2012-06-22 02:30:00     2          4      6        174  \n",
       "5 2012-06-22 03:00:00     3          4      6        174  \n",
       "6 2012-06-22 03:30:00     3          4      6        174  \n",
       "7 2012-06-22 04:00:00     4          4      6        174  \n",
       "8 2012-06-22 04:30:00     4          4      6        174  \n",
       "9 2012-06-22 05:00:00     5          4      6        174  \n",
       "\n",
       "[10 rows x 76 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3cc1c-ea8d-47ce-8ccf-c83ac57349fd",
   "metadata": {},
   "source": [
    "### Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "245501fc-6683-4e93-a008-094b46d4db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for training\n",
    "drivers = ['LE', 'H', 'Tau', 'LE_strg', 'Ta', 'RH', 'VPD', 'Rg', 'Ustar', 'Tsoil1', 'Tsoil2']\n",
    "\n",
    "#-------------------------------------------------\n",
    "# prepare and split data for regressor\n",
    "columns_to_pick = drivers + stat_tags + rg_tag + ['hour', 'dayofweek', 'month', 'dayofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d327751e-f936-4ac2-b15e-8f9baf8695bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split data into training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame with 'Timestamp' and 'NEE' columns.\n",
    "    test_size (float): Proportion of the data to include in the test split.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Training and test DataFrames.\n",
    "    \"\"\"\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n",
    "    return train_data.reset_index(drop=True), test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3921a48-8c56-4701-b953-4ff6d6304a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7983c833-f793-4cb3-aa48-bc4e2ec8e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[columns_to_pick]\n",
    "X_train = pd.get_dummies(X_train, columns=['hour', 'dayofweek', 'month']).values\n",
    "Y_train = train_data[\"NEE\"].values\n",
    "\n",
    "X_test = test_data[columns_to_pick]\n",
    "X_test = pd.get_dummies(X_test, columns=['hour', 'dayofweek', 'month']).values\n",
    "Y_test = test_data[\"NEE\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40200f11-622e-47a3-92b9-469cc0e0fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "947cec65-1c43-4b7c-9c0d-1b702ae45566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(input_shape,), activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dense(8, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"he_normal\"))\n",
    "    model.compile(optimizer=Adam(), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4acb51c7-d0ef-4833-b128-65ec5f022ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 32)                2048      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2721 (10.63 KB)\n",
      "Trainable params: 2721 (10.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(X_train.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1b941-5758-420a-9623-38146e2abdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1552/1552 [==============================] - 1s 300us/step - loss: 12.9340 - val_loss: 11.6164\n",
      "Epoch 2/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 9.4410 - val_loss: 10.9634\n",
      "Epoch 3/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 8.9191 - val_loss: 10.7968\n",
      "Epoch 4/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 8.4694 - val_loss: 11.0584\n",
      "Epoch 5/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 8.1502 - val_loss: 11.2018\n",
      "Epoch 6/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.9371 - val_loss: 11.4305\n",
      "Epoch 7/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.8099 - val_loss: 10.9845\n",
      "Epoch 8/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.6897 - val_loss: 11.2821\n",
      "Epoch 9/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.5527 - val_loss: 11.5059\n",
      "Epoch 10/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 7.4015 - val_loss: 11.5753\n",
      "Epoch 11/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 7.3552 - val_loss: 11.7331\n",
      "Epoch 12/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.2308 - val_loss: 11.7371\n",
      "Epoch 13/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.1567 - val_loss: 11.8184\n",
      "Epoch 14/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 7.0920 - val_loss: 11.8511\n",
      "Epoch 15/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 6.9926 - val_loss: 12.2100\n",
      "Epoch 16/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.9591 - val_loss: 11.8528\n",
      "Epoch 17/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.9029 - val_loss: 11.8145\n",
      "Epoch 18/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 6.8460 - val_loss: 11.8984\n",
      "Epoch 19/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.7666 - val_loss: 11.9353\n",
      "Epoch 20/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.6923 - val_loss: 12.2396\n",
      "Epoch 21/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.6201 - val_loss: 12.1628\n",
      "Epoch 22/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.5821 - val_loss: 11.9376\n",
      "Epoch 23/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 6.5228 - val_loss: 11.9405\n",
      "Epoch 24/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.4701 - val_loss: 11.7872\n",
      "Epoch 25/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.3894 - val_loss: 11.6650\n",
      "Epoch 26/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.2880 - val_loss: 11.4099\n",
      "Epoch 27/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.2488 - val_loss: 11.8530\n",
      "Epoch 28/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 6.1761 - val_loss: 12.1006\n",
      "Epoch 29/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.1177 - val_loss: 11.9734\n",
      "Epoch 30/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.0660 - val_loss: 12.0687\n",
      "Epoch 31/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 6.0242 - val_loss: 12.2740\n",
      "Epoch 32/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 5.9778 - val_loss: 11.7915\n",
      "Epoch 33/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.9193 - val_loss: 12.2128\n",
      "Epoch 34/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.8631 - val_loss: 11.8065\n",
      "Epoch 35/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.8652 - val_loss: 12.0790\n",
      "Epoch 36/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.8206 - val_loss: 12.3090\n",
      "Epoch 37/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.7708 - val_loss: 12.2943\n",
      "Epoch 38/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 5.7384 - val_loss: 12.6429\n",
      "Epoch 39/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.7078 - val_loss: 12.2351\n",
      "Epoch 40/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.6854 - val_loss: 11.8828\n",
      "Epoch 41/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.6540 - val_loss: 12.7488\n",
      "Epoch 42/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.5902 - val_loss: 12.3390\n",
      "Epoch 43/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.5896 - val_loss: 13.4095\n",
      "Epoch 44/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.5584 - val_loss: 12.2305\n",
      "Epoch 45/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.5294 - val_loss: 12.1859\n",
      "Epoch 46/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.5360 - val_loss: 12.5890\n",
      "Epoch 47/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.5021 - val_loss: 12.7701\n",
      "Epoch 48/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.4535 - val_loss: 12.4717\n",
      "Epoch 49/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 5.4165 - val_loss: 11.9937\n",
      "Epoch 50/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.4386 - val_loss: 12.6819\n",
      "Epoch 51/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 5.4058 - val_loss: 13.0787\n",
      "Epoch 52/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.3574 - val_loss: 12.3175\n",
      "Epoch 53/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.3123 - val_loss: 12.5403\n",
      "Epoch 54/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 5.3239 - val_loss: 12.3316\n",
      "Epoch 55/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.2941 - val_loss: 12.5079\n",
      "Epoch 56/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.2852 - val_loss: 12.1986\n",
      "Epoch 57/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.2688 - val_loss: 12.6351\n",
      "Epoch 58/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.2245 - val_loss: 12.7740\n",
      "Epoch 59/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.2415 - val_loss: 12.8959\n",
      "Epoch 60/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 5.1547 - val_loss: 12.3197\n",
      "Epoch 61/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.1760 - val_loss: 12.7069\n",
      "Epoch 62/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 5.1475 - val_loss: 12.5070\n",
      "Epoch 63/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 5.1051 - val_loss: 13.6264\n",
      "Epoch 64/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.1356 - val_loss: 12.1801\n",
      "Epoch 65/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.0771 - val_loss: 12.7997\n",
      "Epoch 66/2000\n",
      "1552/1552 [==============================] - 0s 267us/step - loss: 5.0587 - val_loss: 12.4501\n",
      "Epoch 67/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.0431 - val_loss: 12.0202\n",
      "Epoch 68/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.0450 - val_loss: 13.1008\n",
      "Epoch 69/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.9928 - val_loss: 13.4521\n",
      "Epoch 70/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 5.0034 - val_loss: 12.9627\n",
      "Epoch 71/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.9803 - val_loss: 12.9215\n",
      "Epoch 72/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 4.9782 - val_loss: 14.0318\n",
      "Epoch 73/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 4.9383 - val_loss: 12.3708\n",
      "Epoch 74/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.9534 - val_loss: 13.6694\n",
      "Epoch 75/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.9311 - val_loss: 13.0727\n",
      "Epoch 76/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.9274 - val_loss: 12.5465\n",
      "Epoch 77/2000\n",
      "1552/1552 [==============================] - 1s 332us/step - loss: 4.8920 - val_loss: 13.7878\n",
      "Epoch 78/2000\n",
      "1552/1552 [==============================] - 0s 293us/step - loss: 4.8665 - val_loss: 12.7300\n",
      "Epoch 79/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.8573 - val_loss: 13.1889\n",
      "Epoch 80/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.8495 - val_loss: 12.3381\n",
      "Epoch 81/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.8321 - val_loss: 13.3145\n",
      "Epoch 82/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.8349 - val_loss: 12.7345\n",
      "Epoch 83/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.8118 - val_loss: 13.5392\n",
      "Epoch 84/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.7929 - val_loss: 12.9583\n",
      "Epoch 85/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.7935 - val_loss: 13.9656\n",
      "Epoch 86/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.7797 - val_loss: 13.5828\n",
      "Epoch 87/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.7747 - val_loss: 13.2262\n",
      "Epoch 88/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.7653 - val_loss: 13.3346\n",
      "Epoch 89/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.7592 - val_loss: 13.0900\n",
      "Epoch 90/2000\n",
      "1552/1552 [==============================] - 0s 297us/step - loss: 4.7419 - val_loss: 13.5097\n",
      "Epoch 91/2000\n",
      "1552/1552 [==============================] - 1s 327us/step - loss: 4.7485 - val_loss: 13.6769\n",
      "Epoch 92/2000\n",
      "1552/1552 [==============================] - 0s 302us/step - loss: 4.7466 - val_loss: 13.6458\n",
      "Epoch 93/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.7012 - val_loss: 14.2363\n",
      "Epoch 94/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.7087 - val_loss: 14.7180\n",
      "Epoch 95/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 4.6966 - val_loss: 14.0795\n",
      "Epoch 96/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.6986 - val_loss: 13.3419\n",
      "Epoch 97/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.6896 - val_loss: 14.0865\n",
      "Epoch 98/2000\n",
      "1552/1552 [==============================] - 0s 306us/step - loss: 4.6967 - val_loss: 14.3418\n",
      "Epoch 99/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.6483 - val_loss: 14.3352\n",
      "Epoch 100/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.6894 - val_loss: 14.8701\n",
      "Epoch 101/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.6543 - val_loss: 14.0838\n",
      "Epoch 102/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.6528 - val_loss: 13.4032\n",
      "Epoch 103/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.6356 - val_loss: 13.8525\n",
      "Epoch 104/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.6275 - val_loss: 13.2584\n",
      "Epoch 105/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.6142 - val_loss: 14.1834\n",
      "Epoch 106/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.6185 - val_loss: 13.6094\n",
      "Epoch 107/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.6045 - val_loss: 13.8232\n",
      "Epoch 108/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5931 - val_loss: 13.5459\n",
      "Epoch 109/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5862 - val_loss: 14.4435\n",
      "Epoch 110/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.6029 - val_loss: 14.4750\n",
      "Epoch 111/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5697 - val_loss: 15.0390\n",
      "Epoch 112/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5741 - val_loss: 13.8101\n",
      "Epoch 113/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5513 - val_loss: 15.0443\n",
      "Epoch 114/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5543 - val_loss: 14.3123\n",
      "Epoch 115/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5783 - val_loss: 14.1739\n",
      "Epoch 116/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.5511 - val_loss: 13.9325\n",
      "Epoch 117/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.5401 - val_loss: 14.5642\n",
      "Epoch 118/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.5626 - val_loss: 14.0364\n",
      "Epoch 119/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5194 - val_loss: 13.3347\n",
      "Epoch 120/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.5350 - val_loss: 14.3342\n",
      "Epoch 121/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.5117 - val_loss: 14.2360\n",
      "Epoch 122/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5250 - val_loss: 13.8305\n",
      "Epoch 123/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.4966 - val_loss: 14.7215\n",
      "Epoch 124/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.5254 - val_loss: 14.1877\n",
      "Epoch 125/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5159 - val_loss: 14.7738\n",
      "Epoch 126/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.5044 - val_loss: 14.5864\n",
      "Epoch 127/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.4865 - val_loss: 14.0131\n",
      "Epoch 128/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4823 - val_loss: 14.7161\n",
      "Epoch 129/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4842 - val_loss: 13.8107\n",
      "Epoch 130/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.4736 - val_loss: 14.8933\n",
      "Epoch 131/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4864 - val_loss: 14.7521\n",
      "Epoch 132/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.4756 - val_loss: 15.0250\n",
      "Epoch 133/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4782 - val_loss: 14.6312\n",
      "Epoch 134/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4802 - val_loss: 14.8288\n",
      "Epoch 135/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.4423 - val_loss: 13.8375\n",
      "Epoch 136/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4461 - val_loss: 14.7336\n",
      "Epoch 137/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4629 - val_loss: 14.2121\n",
      "Epoch 138/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4495 - val_loss: 14.0312\n",
      "Epoch 139/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4407 - val_loss: 13.3084\n",
      "Epoch 140/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4512 - val_loss: 14.1011\n",
      "Epoch 141/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4236 - val_loss: 14.5872\n",
      "Epoch 142/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4114 - val_loss: 14.5121\n",
      "Epoch 143/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4194 - val_loss: 14.1856\n",
      "Epoch 144/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 4.4217 - val_loss: 14.7715\n",
      "Epoch 145/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4237 - val_loss: 15.1719\n",
      "Epoch 146/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.4182 - val_loss: 14.0786\n",
      "Epoch 147/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.4126 - val_loss: 14.7199\n",
      "Epoch 148/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3797 - val_loss: 13.5998\n",
      "Epoch 149/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 4.4179 - val_loss: 15.0263\n",
      "Epoch 150/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.3935 - val_loss: 14.8569\n",
      "Epoch 151/2000\n",
      "1552/1552 [==============================] - 0s 295us/step - loss: 4.3715 - val_loss: 14.1883\n",
      "Epoch 152/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 4.4181 - val_loss: 13.7690\n",
      "Epoch 153/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.3890 - val_loss: 14.4801\n",
      "Epoch 154/2000\n",
      "1552/1552 [==============================] - 0s 303us/step - loss: 4.3951 - val_loss: 14.1680\n",
      "Epoch 155/2000\n",
      "1552/1552 [==============================] - 0s 298us/step - loss: 4.3534 - val_loss: 14.2827\n",
      "Epoch 156/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 4.3967 - val_loss: 15.5021\n",
      "Epoch 157/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3590 - val_loss: 13.7696\n",
      "Epoch 158/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3454 - val_loss: 15.0052\n",
      "Epoch 159/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3716 - val_loss: 14.1346\n",
      "Epoch 160/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3458 - val_loss: 13.9090\n",
      "Epoch 161/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.3409 - val_loss: 13.7175\n",
      "Epoch 162/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.3800 - val_loss: 15.2185\n",
      "Epoch 163/2000\n",
      "1552/1552 [==============================] - 0s 304us/step - loss: 4.3276 - val_loss: 16.3350\n",
      "Epoch 164/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.3316 - val_loss: 14.5916\n",
      "Epoch 165/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.3395 - val_loss: 14.2735\n",
      "Epoch 166/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3311 - val_loss: 14.7570\n",
      "Epoch 167/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.3229 - val_loss: 15.0427\n",
      "Epoch 168/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.3272 - val_loss: 14.7538\n",
      "Epoch 169/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.3138 - val_loss: 14.3901\n",
      "Epoch 170/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 4.3185 - val_loss: 14.8716\n",
      "Epoch 171/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.2919 - val_loss: 14.5926\n",
      "Epoch 172/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3067 - val_loss: 14.6845\n",
      "Epoch 173/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.3157 - val_loss: 14.0361\n",
      "Epoch 174/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.3192 - val_loss: 14.9460\n",
      "Epoch 175/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2982 - val_loss: 14.2860\n",
      "Epoch 176/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2919 - val_loss: 14.4832\n",
      "Epoch 177/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2946 - val_loss: 14.3686\n",
      "Epoch 178/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2910 - val_loss: 15.1762\n",
      "Epoch 179/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2836 - val_loss: 14.2224\n",
      "Epoch 180/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2852 - val_loss: 14.1807\n",
      "Epoch 181/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2984 - val_loss: 14.9170\n",
      "Epoch 182/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2966 - val_loss: 15.3428\n",
      "Epoch 183/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.2446 - val_loss: 15.3379\n",
      "Epoch 184/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2859 - val_loss: 15.2335\n",
      "Epoch 185/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2755 - val_loss: 13.8817\n",
      "Epoch 186/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2547 - val_loss: 14.9103\n",
      "Epoch 187/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2678 - val_loss: 14.7092\n",
      "Epoch 188/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2586 - val_loss: 15.3176\n",
      "Epoch 189/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2785 - val_loss: 14.1088\n",
      "Epoch 190/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2614 - val_loss: 14.1039\n",
      "Epoch 191/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2486 - val_loss: 14.2968\n",
      "Epoch 192/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2594 - val_loss: 14.4608\n",
      "Epoch 193/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2539 - val_loss: 14.1365\n",
      "Epoch 194/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2471 - val_loss: 14.4107\n",
      "Epoch 195/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2420 - val_loss: 14.2570\n",
      "Epoch 196/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2514 - val_loss: 14.9776\n",
      "Epoch 197/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2308 - val_loss: 14.4466\n",
      "Epoch 198/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2501 - val_loss: 14.2566\n",
      "Epoch 199/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2260 - val_loss: 15.1203\n",
      "Epoch 200/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2432 - val_loss: 14.6046\n",
      "Epoch 201/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2435 - val_loss: 14.8780\n",
      "Epoch 202/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2358 - val_loss: 15.2455\n",
      "Epoch 203/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2262 - val_loss: 15.0617\n",
      "Epoch 204/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2300 - val_loss: 14.9985\n",
      "Epoch 205/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1970 - val_loss: 15.5410\n",
      "Epoch 206/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.2094 - val_loss: 14.8757\n",
      "Epoch 207/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.2181 - val_loss: 15.4309\n",
      "Epoch 208/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2220 - val_loss: 14.8460\n",
      "Epoch 209/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.2193 - val_loss: 14.3026\n",
      "Epoch 210/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1916 - val_loss: 14.6353\n",
      "Epoch 211/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 4.2075 - val_loss: 14.6024\n",
      "Epoch 212/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2168 - val_loss: 14.7552\n",
      "Epoch 213/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1909 - val_loss: 15.1139\n",
      "Epoch 214/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 4.2091 - val_loss: 15.3585\n",
      "Epoch 215/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.1855 - val_loss: 15.4286\n",
      "Epoch 216/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.2004 - val_loss: 15.5798\n",
      "Epoch 217/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1892 - val_loss: 15.1413\n",
      "Epoch 218/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1845 - val_loss: 14.9856\n",
      "Epoch 219/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2034 - val_loss: 14.6342\n",
      "Epoch 220/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1840 - val_loss: 14.4470\n",
      "Epoch 221/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1944 - val_loss: 16.7311\n",
      "Epoch 222/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1673 - val_loss: 14.5522\n",
      "Epoch 223/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.2012 - val_loss: 14.5197\n",
      "Epoch 224/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1777 - val_loss: 15.1091\n",
      "Epoch 225/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1709 - val_loss: 14.7912\n",
      "Epoch 226/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1872 - val_loss: 14.4790\n",
      "Epoch 227/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1919 - val_loss: 14.9723\n",
      "Epoch 228/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1602 - val_loss: 15.2094\n",
      "Epoch 229/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1672 - val_loss: 15.9968\n",
      "Epoch 230/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1704 - val_loss: 14.3581\n",
      "Epoch 231/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1705 - val_loss: 15.1286\n",
      "Epoch 232/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1723 - val_loss: 14.6679\n",
      "Epoch 233/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 4.1886 - val_loss: 15.1265\n",
      "Epoch 234/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1656 - val_loss: 16.2449\n",
      "Epoch 235/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 4.1599 - val_loss: 14.8507\n",
      "Epoch 236/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1710 - val_loss: 15.2389\n",
      "Epoch 237/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1512 - val_loss: 14.3977\n",
      "Epoch 238/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1553 - val_loss: 15.1597\n",
      "Epoch 239/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1392 - val_loss: 13.9331\n",
      "Epoch 240/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1352 - val_loss: 14.9813\n",
      "Epoch 241/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1563 - val_loss: 14.2708\n",
      "Epoch 242/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1339 - val_loss: 15.4674\n",
      "Epoch 243/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1401 - val_loss: 14.6658\n",
      "Epoch 244/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1609 - val_loss: 16.1729\n",
      "Epoch 245/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1520 - val_loss: 15.1377\n",
      "Epoch 246/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 4.1308 - val_loss: 15.4342\n",
      "Epoch 247/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.1546 - val_loss: 14.7372\n",
      "Epoch 248/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1421 - val_loss: 16.2922\n",
      "Epoch 249/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1277 - val_loss: 15.8589\n",
      "Epoch 250/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1442 - val_loss: 15.1065\n",
      "Epoch 251/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1422 - val_loss: 14.9007\n",
      "Epoch 252/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1421 - val_loss: 15.1077\n",
      "Epoch 253/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1430 - val_loss: 15.2352\n",
      "Epoch 254/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1409 - val_loss: 15.2342\n",
      "Epoch 255/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1269 - val_loss: 15.1444\n",
      "Epoch 256/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 4.1077 - val_loss: 15.1746\n",
      "Epoch 257/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.1189 - val_loss: 14.5109\n",
      "Epoch 258/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1102 - val_loss: 14.5803\n",
      "Epoch 259/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1072 - val_loss: 15.9924\n",
      "Epoch 260/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1331 - val_loss: 15.0484\n",
      "Epoch 261/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.1102 - val_loss: 15.3263\n",
      "Epoch 262/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1089 - val_loss: 16.0969\n",
      "Epoch 263/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1218 - val_loss: 14.7419\n",
      "Epoch 264/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1168 - val_loss: 15.6875\n",
      "Epoch 265/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1036 - val_loss: 14.9139\n",
      "Epoch 266/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0863 - val_loss: 15.4269\n",
      "Epoch 267/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0983 - val_loss: 14.8936\n",
      "Epoch 268/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0862 - val_loss: 15.5441\n",
      "Epoch 269/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.1152 - val_loss: 15.3102\n",
      "Epoch 270/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1144 - val_loss: 15.5283\n",
      "Epoch 271/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 4.0984 - val_loss: 15.7891\n",
      "Epoch 272/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.0997 - val_loss: 15.0886\n",
      "Epoch 273/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1211 - val_loss: 16.3096\n",
      "Epoch 274/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.0959 - val_loss: 14.9767\n",
      "Epoch 275/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.1032 - val_loss: 15.5001\n",
      "Epoch 276/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0872 - val_loss: 17.0515\n",
      "Epoch 277/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0743 - val_loss: 15.6306\n",
      "Epoch 278/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0621 - val_loss: 15.6036\n",
      "Epoch 279/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.1028 - val_loss: 15.9568\n",
      "Epoch 280/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0846 - val_loss: 15.3526\n",
      "Epoch 281/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0749 - val_loss: 15.7028\n",
      "Epoch 282/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 4.0710 - val_loss: 15.5148\n",
      "Epoch 283/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 4.0860 - val_loss: 15.0643\n",
      "Epoch 284/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 4.0768 - val_loss: 15.3574\n",
      "Epoch 285/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.0902 - val_loss: 15.6601\n",
      "Epoch 286/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 4.0767 - val_loss: 15.4765\n",
      "Epoch 287/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0938 - val_loss: 15.0804\n",
      "Epoch 288/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0773 - val_loss: 16.1660\n",
      "Epoch 289/2000\n",
      "1552/1552 [==============================] - 0s 284us/step - loss: 4.0966 - val_loss: 16.2148\n",
      "Epoch 290/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0419 - val_loss: 15.4270\n",
      "Epoch 291/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0661 - val_loss: 15.6145\n",
      "Epoch 292/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0581 - val_loss: 15.1877\n",
      "Epoch 293/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0660 - val_loss: 14.8717\n",
      "Epoch 294/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0717 - val_loss: 14.8468\n",
      "Epoch 295/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 4.0639 - val_loss: 15.0434\n",
      "Epoch 296/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0643 - val_loss: 14.9919\n",
      "Epoch 297/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0234 - val_loss: 15.7053\n",
      "Epoch 298/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0530 - val_loss: 16.4384\n",
      "Epoch 299/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0436 - val_loss: 15.7754\n",
      "Epoch 300/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0483 - val_loss: 15.9764\n",
      "Epoch 301/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0511 - val_loss: 15.6474\n",
      "Epoch 302/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0644 - val_loss: 15.5645\n",
      "Epoch 303/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0383 - val_loss: 16.5789\n",
      "Epoch 304/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0442 - val_loss: 14.3182\n",
      "Epoch 305/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0350 - val_loss: 15.5037\n",
      "Epoch 306/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0393 - val_loss: 15.0343\n",
      "Epoch 307/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0331 - val_loss: 15.1094\n",
      "Epoch 308/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0467 - val_loss: 15.7868\n",
      "Epoch 309/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0400 - val_loss: 15.6018\n",
      "Epoch 310/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0334 - val_loss: 15.4837\n",
      "Epoch 311/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0267 - val_loss: 15.7227\n",
      "Epoch 312/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0308 - val_loss: 15.2364\n",
      "Epoch 313/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0285 - val_loss: 15.1033\n",
      "Epoch 314/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0427 - val_loss: 16.2564\n",
      "Epoch 315/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0181 - val_loss: 15.9994\n",
      "Epoch 316/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0343 - val_loss: 15.5912\n",
      "Epoch 317/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 4.0203 - val_loss: 16.6621\n",
      "Epoch 318/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0199 - val_loss: 15.4378\n",
      "Epoch 319/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0261 - val_loss: 15.1946\n",
      "Epoch 320/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0258 - val_loss: 15.3107\n",
      "Epoch 321/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0459 - val_loss: 15.5740\n",
      "Epoch 322/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0348 - val_loss: 15.9063\n",
      "Epoch 323/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 4.0201 - val_loss: 15.4917\n",
      "Epoch 324/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0283 - val_loss: 15.0700\n",
      "Epoch 325/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0104 - val_loss: 15.2482\n",
      "Epoch 326/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0047 - val_loss: 14.6766\n",
      "Epoch 327/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9989 - val_loss: 15.2529\n",
      "Epoch 328/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0159 - val_loss: 15.5279\n",
      "Epoch 329/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0281 - val_loss: 15.4948\n",
      "Epoch 330/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9988 - val_loss: 15.6108\n",
      "Epoch 331/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0083 - val_loss: 16.0706\n",
      "Epoch 332/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0088 - val_loss: 14.7890\n",
      "Epoch 333/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9807 - val_loss: 15.5168\n",
      "Epoch 334/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0121 - val_loss: 16.0833\n",
      "Epoch 335/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 4.0095 - val_loss: 15.3989\n",
      "Epoch 336/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0053 - val_loss: 15.6143\n",
      "Epoch 337/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9970 - val_loss: 15.9353\n",
      "Epoch 338/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9934 - val_loss: 15.0958\n",
      "Epoch 339/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 4.0091 - val_loss: 16.1796\n",
      "Epoch 340/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 4.0009 - val_loss: 16.3174\n",
      "Epoch 341/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0168 - val_loss: 14.9591\n",
      "Epoch 342/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9807 - val_loss: 15.4146\n",
      "Epoch 343/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9871 - val_loss: 14.8105\n",
      "Epoch 344/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 4.0080 - val_loss: 15.8307\n",
      "Epoch 345/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9856 - val_loss: 15.4902\n",
      "Epoch 346/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9881 - val_loss: 14.8753\n",
      "Epoch 347/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9854 - val_loss: 16.1848\n",
      "Epoch 348/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9986 - val_loss: 15.6572\n",
      "Epoch 349/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9891 - val_loss: 15.3315\n",
      "Epoch 350/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9944 - val_loss: 15.6157\n",
      "Epoch 351/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9816 - val_loss: 16.3513\n",
      "Epoch 352/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9898 - val_loss: 15.6453\n",
      "Epoch 353/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9767 - val_loss: 15.9504\n",
      "Epoch 354/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9949 - val_loss: 15.8640\n",
      "Epoch 355/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9610 - val_loss: 16.0179\n",
      "Epoch 356/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9661 - val_loss: 15.7440\n",
      "Epoch 357/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9713 - val_loss: 14.8635\n",
      "Epoch 358/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9821 - val_loss: 14.9225\n",
      "Epoch 359/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9850 - val_loss: 16.0730\n",
      "Epoch 360/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9743 - val_loss: 16.4025\n",
      "Epoch 361/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9699 - val_loss: 16.1198\n",
      "Epoch 362/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9646 - val_loss: 15.6376\n",
      "Epoch 363/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9657 - val_loss: 16.0138\n",
      "Epoch 364/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9759 - val_loss: 15.9571\n",
      "Epoch 365/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9604 - val_loss: 16.3058\n",
      "Epoch 366/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9762 - val_loss: 15.8786\n",
      "Epoch 367/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9466 - val_loss: 15.1869\n",
      "Epoch 368/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9597 - val_loss: 15.3073\n",
      "Epoch 369/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9538 - val_loss: 16.9313\n",
      "Epoch 370/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9588 - val_loss: 16.1217\n",
      "Epoch 371/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9640 - val_loss: 16.2929\n",
      "Epoch 372/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9806 - val_loss: 15.6837\n",
      "Epoch 373/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9479 - val_loss: 15.9067\n",
      "Epoch 374/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9477 - val_loss: 16.0138\n",
      "Epoch 375/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9381 - val_loss: 15.3894\n",
      "Epoch 376/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9678 - val_loss: 15.0220\n",
      "Epoch 377/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9701 - val_loss: 15.4867\n",
      "Epoch 378/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9618 - val_loss: 15.1517\n",
      "Epoch 379/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9450 - val_loss: 16.2357\n",
      "Epoch 380/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9509 - val_loss: 15.9293\n",
      "Epoch 381/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9484 - val_loss: 15.2748\n",
      "Epoch 382/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9559 - val_loss: 15.9912\n",
      "Epoch 383/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9604 - val_loss: 15.5591\n",
      "Epoch 384/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9530 - val_loss: 15.0406\n",
      "Epoch 385/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9542 - val_loss: 15.3018\n",
      "Epoch 386/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9621 - val_loss: 15.7700\n",
      "Epoch 387/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9489 - val_loss: 16.1263\n",
      "Epoch 388/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9540 - val_loss: 15.6416\n",
      "Epoch 389/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9321 - val_loss: 15.9298\n",
      "Epoch 390/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9538 - val_loss: 15.8364\n",
      "Epoch 391/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.9532 - val_loss: 15.7949\n",
      "Epoch 392/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9315 - val_loss: 16.1572\n",
      "Epoch 393/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9310 - val_loss: 14.9434\n",
      "Epoch 394/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9404 - val_loss: 15.7299\n",
      "Epoch 395/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9240 - val_loss: 16.3075\n",
      "Epoch 396/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9373 - val_loss: 15.8225\n",
      "Epoch 397/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9332 - val_loss: 15.4401\n",
      "Epoch 398/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9454 - val_loss: 15.5198\n",
      "Epoch 399/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9377 - val_loss: 15.7038\n",
      "Epoch 400/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9257 - val_loss: 15.9348\n",
      "Epoch 401/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9457 - val_loss: 15.7749\n",
      "Epoch 402/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9334 - val_loss: 15.3631\n",
      "Epoch 403/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9446 - val_loss: 15.9443\n",
      "Epoch 404/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9207 - val_loss: 15.2083\n",
      "Epoch 405/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9301 - val_loss: 16.0459\n",
      "Epoch 406/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9174 - val_loss: 15.7727\n",
      "Epoch 407/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.9170 - val_loss: 15.9341\n",
      "Epoch 408/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9367 - val_loss: 15.3356\n",
      "Epoch 409/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9356 - val_loss: 16.1674\n",
      "Epoch 410/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9326 - val_loss: 14.8883\n",
      "Epoch 411/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9373 - val_loss: 16.1973\n",
      "Epoch 412/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9265 - val_loss: 15.3051\n",
      "Epoch 413/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9344 - val_loss: 15.8379\n",
      "Epoch 414/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9241 - val_loss: 16.3099\n",
      "Epoch 415/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9246 - val_loss: 15.9409\n",
      "Epoch 416/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9132 - val_loss: 16.0826\n",
      "Epoch 417/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9247 - val_loss: 16.0021\n",
      "Epoch 418/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.9220 - val_loss: 15.9946\n",
      "Epoch 419/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9193 - val_loss: 16.1417\n",
      "Epoch 420/2000\n",
      "1552/1552 [==============================] - 0s 312us/step - loss: 3.9255 - val_loss: 15.0501\n",
      "Epoch 421/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.9140 - val_loss: 15.8940\n",
      "Epoch 422/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9223 - val_loss: 15.4355\n",
      "Epoch 423/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9273 - val_loss: 15.7150\n",
      "Epoch 424/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9029 - val_loss: 15.9837\n",
      "Epoch 425/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9045 - val_loss: 15.8013\n",
      "Epoch 426/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9318 - val_loss: 15.5693\n",
      "Epoch 427/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9034 - val_loss: 17.1077\n",
      "Epoch 428/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.9169 - val_loss: 15.6843\n",
      "Epoch 429/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9165 - val_loss: 15.7593\n",
      "Epoch 430/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.9235 - val_loss: 15.7892\n",
      "Epoch 431/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.9116 - val_loss: 16.3870\n",
      "Epoch 432/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9328 - val_loss: 15.6609\n",
      "Epoch 433/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9086 - val_loss: 16.8703\n",
      "Epoch 434/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9061 - val_loss: 15.6524\n",
      "Epoch 435/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.9254 - val_loss: 14.8841\n",
      "Epoch 436/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8889 - val_loss: 16.3089\n",
      "Epoch 437/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8903 - val_loss: 16.5209\n",
      "Epoch 438/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8928 - val_loss: 15.9125\n",
      "Epoch 439/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8882 - val_loss: 15.9724\n",
      "Epoch 440/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.9156 - val_loss: 15.5913\n",
      "Epoch 441/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.9001 - val_loss: 15.7999\n",
      "Epoch 442/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.9040 - val_loss: 16.4302\n",
      "Epoch 443/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8850 - val_loss: 15.9117\n",
      "Epoch 444/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.9031 - val_loss: 15.4888\n",
      "Epoch 445/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9047 - val_loss: 16.4354\n",
      "Epoch 446/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8915 - val_loss: 16.0067\n",
      "Epoch 447/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8882 - val_loss: 15.8567\n",
      "Epoch 448/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8954 - val_loss: 16.0713\n",
      "Epoch 449/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9048 - val_loss: 15.1317\n",
      "Epoch 450/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.9004 - val_loss: 15.6190\n",
      "Epoch 451/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8864 - val_loss: 15.9061\n",
      "Epoch 452/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8826 - val_loss: 15.8548\n",
      "Epoch 453/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8840 - val_loss: 16.0364\n",
      "Epoch 454/2000\n",
      "1552/1552 [==============================] - 1s 323us/step - loss: 3.8991 - val_loss: 15.7910\n",
      "Epoch 455/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8790 - val_loss: 15.2535\n",
      "Epoch 456/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8993 - val_loss: 15.2197\n",
      "Epoch 457/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8948 - val_loss: 16.3301\n",
      "Epoch 458/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.9005 - val_loss: 16.1773\n",
      "Epoch 459/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8764 - val_loss: 16.5004\n",
      "Epoch 460/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8790 - val_loss: 15.8706\n",
      "Epoch 461/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8856 - val_loss: 15.5004\n",
      "Epoch 462/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8955 - val_loss: 16.0213\n",
      "Epoch 463/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8750 - val_loss: 15.6002\n",
      "Epoch 464/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8789 - val_loss: 16.1065\n",
      "Epoch 465/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8866 - val_loss: 16.1983\n",
      "Epoch 466/2000\n",
      "1552/1552 [==============================] - 0s 300us/step - loss: 3.8914 - val_loss: 16.8446\n",
      "Epoch 467/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8776 - val_loss: 16.4457\n",
      "Epoch 468/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8759 - val_loss: 16.0048\n",
      "Epoch 469/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.8831 - val_loss: 15.7607\n",
      "Epoch 470/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.8616 - val_loss: 16.8586\n",
      "Epoch 471/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.8974 - val_loss: 16.1549\n",
      "Epoch 472/2000\n",
      "1552/1552 [==============================] - 1s 575us/step - loss: 3.8682 - val_loss: 16.0833\n",
      "Epoch 473/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.8699 - val_loss: 16.3256\n",
      "Epoch 474/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8862 - val_loss: 15.8466\n",
      "Epoch 475/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8799 - val_loss: 15.6047\n",
      "Epoch 476/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8627 - val_loss: 15.6296\n",
      "Epoch 477/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8788 - val_loss: 16.1885\n",
      "Epoch 478/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8745 - val_loss: 15.3321\n",
      "Epoch 479/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8464 - val_loss: 16.4295\n",
      "Epoch 480/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8772 - val_loss: 15.3664\n",
      "Epoch 481/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8661 - val_loss: 15.5506\n",
      "Epoch 482/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8547 - val_loss: 15.6248\n",
      "Epoch 483/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8761 - val_loss: 16.5505\n",
      "Epoch 484/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8676 - val_loss: 15.6643\n",
      "Epoch 485/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.8632 - val_loss: 16.2087\n",
      "Epoch 486/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.8523 - val_loss: 15.5534\n",
      "Epoch 487/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8769 - val_loss: 15.9327\n",
      "Epoch 488/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8470 - val_loss: 16.1503\n",
      "Epoch 489/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8775 - val_loss: 15.6049\n",
      "Epoch 490/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8486 - val_loss: 15.9664\n",
      "Epoch 491/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8589 - val_loss: 16.2315\n",
      "Epoch 492/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8584 - val_loss: 16.3572\n",
      "Epoch 493/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8628 - val_loss: 16.7636\n",
      "Epoch 494/2000\n",
      "1552/1552 [==============================] - 0s 284us/step - loss: 3.8542 - val_loss: 16.7117\n",
      "Epoch 495/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8826 - val_loss: 15.7887\n",
      "Epoch 496/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8561 - val_loss: 15.4419\n",
      "Epoch 497/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8725 - val_loss: 14.9189\n",
      "Epoch 498/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8554 - val_loss: 15.6825\n",
      "Epoch 499/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8455 - val_loss: 16.6867\n",
      "Epoch 500/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8566 - val_loss: 15.7915\n",
      "Epoch 501/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8599 - val_loss: 16.4935\n",
      "Epoch 502/2000\n",
      "1552/1552 [==============================] - 1s 495us/step - loss: 3.8630 - val_loss: 16.3421\n",
      "Epoch 503/2000\n",
      "1552/1552 [==============================] - 1s 379us/step - loss: 3.8345 - val_loss: 15.6191\n",
      "Epoch 504/2000\n",
      "1552/1552 [==============================] - 1s 367us/step - loss: 3.8540 - val_loss: 17.0697\n",
      "Epoch 505/2000\n",
      "1552/1552 [==============================] - 0s 286us/step - loss: 3.8427 - val_loss: 16.8096\n",
      "Epoch 506/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.8705 - val_loss: 16.1435\n",
      "Epoch 507/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8443 - val_loss: 16.0475\n",
      "Epoch 508/2000\n",
      "1552/1552 [==============================] - 1s 344us/step - loss: 3.9053 - val_loss: 16.2957\n",
      "Epoch 509/2000\n",
      "1552/1552 [==============================] - 1s 353us/step - loss: 3.8393 - val_loss: 15.5650\n",
      "Epoch 510/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8411 - val_loss: 16.6007\n",
      "Epoch 511/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8437 - val_loss: 16.2036\n",
      "Epoch 512/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8348 - val_loss: 16.0989\n",
      "Epoch 513/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8369 - val_loss: 16.4037\n",
      "Epoch 514/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.9009 - val_loss: 16.2376\n",
      "Epoch 515/2000\n",
      "1552/1552 [==============================] - 0s 318us/step - loss: 3.8321 - val_loss: 15.4512\n",
      "Epoch 516/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8615 - val_loss: 16.2348\n",
      "Epoch 517/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8462 - val_loss: 17.5829\n",
      "Epoch 518/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.8289 - val_loss: 16.8544\n",
      "Epoch 519/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8427 - val_loss: 16.4049\n",
      "Epoch 520/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8327 - val_loss: 16.3499\n",
      "Epoch 521/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8449 - val_loss: 16.0501\n",
      "Epoch 522/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.8383 - val_loss: 16.0141\n",
      "Epoch 523/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8409 - val_loss: 17.0759\n",
      "Epoch 524/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8349 - val_loss: 16.0569\n",
      "Epoch 525/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8520 - val_loss: 16.5335\n",
      "Epoch 526/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.8366 - val_loss: 16.0630\n",
      "Epoch 527/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8443 - val_loss: 16.0225\n",
      "Epoch 528/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8155 - val_loss: 16.1537\n",
      "Epoch 529/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8497 - val_loss: 15.9458\n",
      "Epoch 530/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8356 - val_loss: 16.6406\n",
      "Epoch 531/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8318 - val_loss: 16.2557\n",
      "Epoch 532/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8213 - val_loss: 16.5695\n",
      "Epoch 533/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8249 - val_loss: 16.0551\n",
      "Epoch 534/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8366 - val_loss: 15.7063\n",
      "Epoch 535/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8415 - val_loss: 15.9424\n",
      "Epoch 536/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8314 - val_loss: 15.7858\n",
      "Epoch 537/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8266 - val_loss: 16.3360\n",
      "Epoch 538/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8180 - val_loss: 15.9645\n",
      "Epoch 539/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8205 - val_loss: 16.4508\n",
      "Epoch 540/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8345 - val_loss: 16.4949\n",
      "Epoch 541/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8114 - val_loss: 16.9794\n",
      "Epoch 542/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8329 - val_loss: 15.9492\n",
      "Epoch 543/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8170 - val_loss: 15.6340\n",
      "Epoch 544/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8390 - val_loss: 15.9859\n",
      "Epoch 545/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8188 - val_loss: 16.1088\n",
      "Epoch 546/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.8145 - val_loss: 16.1502\n",
      "Epoch 547/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8262 - val_loss: 15.9905\n",
      "Epoch 548/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8365 - val_loss: 16.9837\n",
      "Epoch 549/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8165 - val_loss: 16.9826\n",
      "Epoch 550/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8091 - val_loss: 16.3446\n",
      "Epoch 551/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8141 - val_loss: 16.0777\n",
      "Epoch 552/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.8400 - val_loss: 16.2003\n",
      "Epoch 553/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8130 - val_loss: 15.7212\n",
      "Epoch 554/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.8291 - val_loss: 17.0328\n",
      "Epoch 555/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8154 - val_loss: 16.0746\n",
      "Epoch 556/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8207 - val_loss: 15.6699\n",
      "Epoch 557/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8239 - val_loss: 16.4101\n",
      "Epoch 558/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8280 - val_loss: 17.1535\n",
      "Epoch 559/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8202 - val_loss: 15.7085\n",
      "Epoch 560/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8078 - val_loss: 16.1459\n",
      "Epoch 561/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8230 - val_loss: 16.9951\n",
      "Epoch 562/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8721 - val_loss: 17.0522\n",
      "Epoch 563/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.8241 - val_loss: 16.2198\n",
      "Epoch 564/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8114 - val_loss: 15.9543\n",
      "Epoch 565/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8529 - val_loss: 16.3006\n",
      "Epoch 566/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8066 - val_loss: 16.4622\n",
      "Epoch 567/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8075 - val_loss: 17.2906\n",
      "Epoch 568/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8154 - val_loss: 16.7526\n",
      "Epoch 569/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8229 - val_loss: 15.8925\n",
      "Epoch 570/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8085 - val_loss: 16.5894\n",
      "Epoch 571/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8312 - val_loss: 16.4364\n",
      "Epoch 572/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8271 - val_loss: 16.1011\n",
      "Epoch 573/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8104 - val_loss: 15.9233\n",
      "Epoch 574/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8103 - val_loss: 16.5151\n",
      "Epoch 575/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.8191 - val_loss: 16.3316\n",
      "Epoch 576/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.8043 - val_loss: 16.5636\n",
      "Epoch 577/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8127 - val_loss: 17.2830\n",
      "Epoch 578/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8171 - val_loss: 16.0036\n",
      "Epoch 579/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7935 - val_loss: 16.2880\n",
      "Epoch 580/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8064 - val_loss: 15.5560\n",
      "Epoch 581/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.8141 - val_loss: 17.1412\n",
      "Epoch 582/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8116 - val_loss: 16.3928\n",
      "Epoch 583/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8026 - val_loss: 16.1200\n",
      "Epoch 584/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8067 - val_loss: 17.1497\n",
      "Epoch 585/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.8105 - val_loss: 16.0396\n",
      "Epoch 586/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.7956 - val_loss: 16.6826\n",
      "Epoch 587/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7906 - val_loss: 16.2948\n",
      "Epoch 588/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.8095 - val_loss: 16.1542\n",
      "Epoch 589/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8017 - val_loss: 16.4231\n",
      "Epoch 590/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8193 - val_loss: 16.5002\n",
      "Epoch 591/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7929 - val_loss: 15.7000\n",
      "Epoch 592/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.8007 - val_loss: 17.2826\n",
      "Epoch 593/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8124 - val_loss: 16.7070\n",
      "Epoch 594/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8075 - val_loss: 16.0355\n",
      "Epoch 595/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7867 - val_loss: 16.0619\n",
      "Epoch 596/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8137 - val_loss: 16.3714\n",
      "Epoch 597/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7867 - val_loss: 16.1963\n",
      "Epoch 598/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7998 - val_loss: 17.8457\n",
      "Epoch 599/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7918 - val_loss: 16.8931\n",
      "Epoch 600/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7869 - val_loss: 16.9435\n",
      "Epoch 601/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8024 - val_loss: 17.3307\n",
      "Epoch 602/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7935 - val_loss: 16.1921\n",
      "Epoch 603/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8077 - val_loss: 16.0034\n",
      "Epoch 604/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7867 - val_loss: 16.8223\n",
      "Epoch 605/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7877 - val_loss: 16.4444\n",
      "Epoch 606/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7887 - val_loss: 16.1086\n",
      "Epoch 607/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8003 - val_loss: 16.3268\n",
      "Epoch 608/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7943 - val_loss: 16.2018\n",
      "Epoch 609/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7936 - val_loss: 16.5348\n",
      "Epoch 610/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.7964 - val_loss: 16.9071\n",
      "Epoch 611/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.8025 - val_loss: 15.5843\n",
      "Epoch 612/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7880 - val_loss: 16.0416\n",
      "Epoch 613/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7958 - val_loss: 16.8342\n",
      "Epoch 614/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7897 - val_loss: 16.7961\n",
      "Epoch 615/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7994 - val_loss: 17.3441\n",
      "Epoch 616/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7804 - val_loss: 16.5787\n",
      "Epoch 617/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.8058 - val_loss: 15.8693\n",
      "Epoch 618/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7865 - val_loss: 16.0945\n",
      "Epoch 619/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7857 - val_loss: 17.0355\n",
      "Epoch 620/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7841 - val_loss: 16.4322\n",
      "Epoch 621/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7956 - val_loss: 16.2079\n",
      "Epoch 622/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.8028 - val_loss: 16.1888\n",
      "Epoch 623/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7663 - val_loss: 16.8691\n",
      "Epoch 624/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8028 - val_loss: 16.6490\n",
      "Epoch 625/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7778 - val_loss: 16.7725\n",
      "Epoch 626/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7921 - val_loss: 16.2788\n",
      "Epoch 627/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7870 - val_loss: 16.6153\n",
      "Epoch 628/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7740 - val_loss: 16.6149\n",
      "Epoch 629/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7792 - val_loss: 16.4447\n",
      "Epoch 630/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7757 - val_loss: 17.1714\n",
      "Epoch 631/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7950 - val_loss: 16.2273\n",
      "Epoch 632/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.8017 - val_loss: 16.1224\n",
      "Epoch 633/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7877 - val_loss: 16.0664\n",
      "Epoch 634/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7690 - val_loss: 16.1267\n",
      "Epoch 635/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7766 - val_loss: 16.1349\n",
      "Epoch 636/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7714 - val_loss: 16.9678\n",
      "Epoch 637/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7950 - val_loss: 16.2374\n",
      "Epoch 638/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7811 - val_loss: 15.9147\n",
      "Epoch 639/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7800 - val_loss: 16.5554\n",
      "Epoch 640/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7695 - val_loss: 17.0488\n",
      "Epoch 641/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7856 - val_loss: 16.4718\n",
      "Epoch 642/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7684 - val_loss: 16.5146\n",
      "Epoch 643/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.7791 - val_loss: 16.2140\n",
      "Epoch 644/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7944 - val_loss: 16.2758\n",
      "Epoch 645/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7701 - val_loss: 16.5630\n",
      "Epoch 646/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7788 - val_loss: 16.7910\n",
      "Epoch 647/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7662 - val_loss: 17.0185\n",
      "Epoch 648/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7654 - val_loss: 16.7978\n",
      "Epoch 649/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7852 - val_loss: 16.3969\n",
      "Epoch 650/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7409 - val_loss: 16.4295\n",
      "Epoch 651/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7811 - val_loss: 17.4906\n",
      "Epoch 652/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7806 - val_loss: 17.1223\n",
      "Epoch 653/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7751 - val_loss: 16.7420\n",
      "Epoch 654/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7615 - val_loss: 16.4793\n",
      "Epoch 655/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7689 - val_loss: 16.3835\n",
      "Epoch 656/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7625 - val_loss: 16.6117\n",
      "Epoch 657/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7742 - val_loss: 16.4477\n",
      "Epoch 658/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7669 - val_loss: 16.4058\n",
      "Epoch 659/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7586 - val_loss: 16.1642\n",
      "Epoch 660/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7733 - val_loss: 16.4192\n",
      "Epoch 661/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7537 - val_loss: 16.5504\n",
      "Epoch 662/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7652 - val_loss: 16.4548\n",
      "Epoch 663/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7648 - val_loss: 17.1788\n",
      "Epoch 664/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7610 - val_loss: 16.8212\n",
      "Epoch 665/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7969 - val_loss: 16.2904\n",
      "Epoch 666/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7677 - val_loss: 16.3346\n",
      "Epoch 667/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7757 - val_loss: 16.2459\n",
      "Epoch 668/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7567 - val_loss: 16.4513\n",
      "Epoch 669/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7683 - val_loss: 16.8963\n",
      "Epoch 670/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7719 - val_loss: 16.2355\n",
      "Epoch 671/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7528 - val_loss: 16.4619\n",
      "Epoch 672/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7852 - val_loss: 17.4785\n",
      "Epoch 673/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7551 - val_loss: 16.1373\n",
      "Epoch 674/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7713 - val_loss: 16.1318\n",
      "Epoch 675/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7723 - val_loss: 16.0595\n",
      "Epoch 676/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7487 - val_loss: 16.3828\n",
      "Epoch 677/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7643 - val_loss: 16.3744\n",
      "Epoch 678/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7801 - val_loss: 16.8069\n",
      "Epoch 679/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7448 - val_loss: 15.8956\n",
      "Epoch 680/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7646 - val_loss: 16.3541\n",
      "Epoch 681/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7595 - val_loss: 16.9575\n",
      "Epoch 682/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7482 - val_loss: 16.9872\n",
      "Epoch 683/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7525 - val_loss: 16.4001\n",
      "Epoch 684/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7777 - val_loss: 15.8218\n",
      "Epoch 685/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7575 - val_loss: 16.8238\n",
      "Epoch 686/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7537 - val_loss: 16.3717\n",
      "Epoch 687/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7794 - val_loss: 17.8478\n",
      "Epoch 688/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7703 - val_loss: 16.0489\n",
      "Epoch 689/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7868 - val_loss: 16.0431\n",
      "Epoch 690/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7612 - val_loss: 16.8206\n",
      "Epoch 691/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7772 - val_loss: 16.7706\n",
      "Epoch 692/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7552 - val_loss: 15.9222\n",
      "Epoch 693/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7638 - val_loss: 18.1545\n",
      "Epoch 694/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.7505 - val_loss: 16.5559\n",
      "Epoch 695/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7544 - val_loss: 16.3001\n",
      "Epoch 696/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7874 - val_loss: 17.2378\n",
      "Epoch 697/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7658 - val_loss: 16.1969\n",
      "Epoch 698/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7479 - val_loss: 16.7001\n",
      "Epoch 699/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7556 - val_loss: 16.7058\n",
      "Epoch 700/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7583 - val_loss: 16.7883\n",
      "Epoch 701/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7531 - val_loss: 16.6125\n",
      "Epoch 702/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7518 - val_loss: 16.5302\n",
      "Epoch 703/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7646 - val_loss: 16.3804\n",
      "Epoch 704/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7634 - val_loss: 17.1601\n",
      "Epoch 705/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7632 - val_loss: 16.7545\n",
      "Epoch 706/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7439 - val_loss: 16.9558\n",
      "Epoch 707/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7561 - val_loss: 16.9112\n",
      "Epoch 708/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7514 - val_loss: 15.8425\n",
      "Epoch 709/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7441 - val_loss: 16.2665\n",
      "Epoch 710/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7699 - val_loss: 17.0877\n",
      "Epoch 711/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7649 - val_loss: 17.0002\n",
      "Epoch 712/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7564 - val_loss: 16.8692\n",
      "Epoch 713/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7445 - val_loss: 15.8820\n",
      "Epoch 714/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7412 - val_loss: 17.5350\n",
      "Epoch 715/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7463 - val_loss: 16.8094\n",
      "Epoch 716/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7561 - val_loss: 16.8874\n",
      "Epoch 717/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7501 - val_loss: 16.6777\n",
      "Epoch 718/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.7369 - val_loss: 17.1832\n",
      "Epoch 719/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.7477 - val_loss: 16.3745\n",
      "Epoch 720/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7558 - val_loss: 16.2101\n",
      "Epoch 721/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7450 - val_loss: 16.1704\n",
      "Epoch 722/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7655 - val_loss: 16.6913\n",
      "Epoch 723/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7617 - val_loss: 16.7551\n",
      "Epoch 724/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7428 - val_loss: 17.4402\n",
      "Epoch 725/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7712 - val_loss: 16.0281\n",
      "Epoch 726/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7448 - val_loss: 17.2619\n",
      "Epoch 727/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7471 - val_loss: 16.1122\n",
      "Epoch 728/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7432 - val_loss: 16.3574\n",
      "Epoch 729/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7346 - val_loss: 16.9666\n",
      "Epoch 730/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7274 - val_loss: 16.1398\n",
      "Epoch 731/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7494 - val_loss: 17.7700\n",
      "Epoch 732/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7343 - val_loss: 16.7515\n",
      "Epoch 733/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7283 - val_loss: 16.7324\n",
      "Epoch 734/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7472 - val_loss: 17.3403\n",
      "Epoch 735/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7411 - val_loss: 16.8164\n",
      "Epoch 736/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7455 - val_loss: 15.9217\n",
      "Epoch 737/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7374 - val_loss: 16.0742\n",
      "Epoch 738/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7469 - val_loss: 16.7681\n",
      "Epoch 739/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7340 - val_loss: 17.0255\n",
      "Epoch 740/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7493 - val_loss: 18.0690\n",
      "Epoch 741/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7450 - val_loss: 16.7899\n",
      "Epoch 742/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7425 - val_loss: 17.0056\n",
      "Epoch 743/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7307 - val_loss: 16.6715\n",
      "Epoch 744/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7334 - val_loss: 16.3822\n",
      "Epoch 745/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7360 - val_loss: 16.0345\n",
      "Epoch 746/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7390 - val_loss: 17.7204\n",
      "Epoch 747/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7452 - val_loss: 16.5881\n",
      "Epoch 748/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7482 - val_loss: 15.8482\n",
      "Epoch 749/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7397 - val_loss: 16.4212\n",
      "Epoch 750/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7446 - val_loss: 16.6135\n",
      "Epoch 751/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7417 - val_loss: 16.3170\n",
      "Epoch 752/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7440 - val_loss: 15.6901\n",
      "Epoch 753/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7519 - val_loss: 16.3980\n",
      "Epoch 754/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7322 - val_loss: 17.0220\n",
      "Epoch 755/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7276 - val_loss: 18.4263\n",
      "Epoch 756/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7231 - val_loss: 17.7019\n",
      "Epoch 757/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7427 - val_loss: 16.2666\n",
      "Epoch 758/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7234 - val_loss: 17.1596\n",
      "Epoch 759/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7302 - val_loss: 17.5528\n",
      "Epoch 760/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7276 - val_loss: 16.2110\n",
      "Epoch 761/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7259 - val_loss: 16.6005\n",
      "Epoch 762/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7276 - val_loss: 16.2102\n",
      "Epoch 763/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7312 - val_loss: 16.3406\n",
      "Epoch 764/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7293 - val_loss: 16.8696\n",
      "Epoch 765/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7197 - val_loss: 17.2984\n",
      "Epoch 766/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7298 - val_loss: 16.5077\n",
      "Epoch 767/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7358 - val_loss: 16.6379\n",
      "Epoch 768/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7389 - val_loss: 16.4952\n",
      "Epoch 769/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7207 - val_loss: 17.6622\n",
      "Epoch 770/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7259 - val_loss: 17.1720\n",
      "Epoch 771/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7335 - val_loss: 16.4257\n",
      "Epoch 772/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7253 - val_loss: 16.5981\n",
      "Epoch 773/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7258 - val_loss: 17.4568\n",
      "Epoch 774/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7401 - val_loss: 17.3575\n",
      "Epoch 775/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7292 - val_loss: 16.5729\n",
      "Epoch 776/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7264 - val_loss: 16.4312\n",
      "Epoch 777/2000\n",
      "1552/1552 [==============================] - 0s 293us/step - loss: 3.7244 - val_loss: 16.9259\n",
      "Epoch 778/2000\n",
      "1552/1552 [==============================] - 0s 266us/step - loss: 3.7280 - val_loss: 17.0413\n",
      "Epoch 779/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7449 - val_loss: 16.8312\n",
      "Epoch 780/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7157 - val_loss: 16.5415\n",
      "Epoch 781/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7442 - val_loss: 17.6512\n",
      "Epoch 782/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7135 - val_loss: 17.7339\n",
      "Epoch 783/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7115 - val_loss: 16.3693\n",
      "Epoch 784/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7204 - val_loss: 17.3392\n",
      "Epoch 785/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7288 - val_loss: 16.1068\n",
      "Epoch 786/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7186 - val_loss: 16.6641\n",
      "Epoch 787/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.7326 - val_loss: 16.9533\n",
      "Epoch 788/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7215 - val_loss: 16.6532\n",
      "Epoch 789/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7172 - val_loss: 17.9469\n",
      "Epoch 790/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7294 - val_loss: 17.1000\n",
      "Epoch 791/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7457 - val_loss: 17.1345\n",
      "Epoch 792/2000\n",
      "1552/1552 [==============================] - 43s 28ms/step - loss: 3.7360 - val_loss: 17.2284\n",
      "Epoch 793/2000\n",
      "1552/1552 [==============================] - 0s 307us/step - loss: 3.7272 - val_loss: 16.5392\n",
      "Epoch 794/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.7183 - val_loss: 16.9142\n",
      "Epoch 795/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7159 - val_loss: 16.7982\n",
      "Epoch 796/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7531 - val_loss: 16.7068\n",
      "Epoch 797/2000\n",
      "1552/1552 [==============================] - 0s 291us/step - loss: 3.7175 - val_loss: 16.7194\n",
      "Epoch 798/2000\n",
      "1552/1552 [==============================] - 0s 303us/step - loss: 3.7184 - val_loss: 16.5025\n",
      "Epoch 799/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7177 - val_loss: 16.1533\n",
      "Epoch 800/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7093 - val_loss: 16.7117\n",
      "Epoch 801/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7105 - val_loss: 16.0399\n",
      "Epoch 802/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.7245 - val_loss: 16.9502\n",
      "Epoch 803/2000\n",
      "1552/1552 [==============================] - 1s 337us/step - loss: 3.7261 - val_loss: 16.5643\n",
      "Epoch 804/2000\n",
      "1552/1552 [==============================] - 0s 307us/step - loss: 3.7021 - val_loss: 16.7395\n",
      "Epoch 805/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7206 - val_loss: 16.3853\n",
      "Epoch 806/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.7249 - val_loss: 16.7108\n",
      "Epoch 807/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.7266 - val_loss: 16.0182\n",
      "Epoch 808/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7283 - val_loss: 16.5358\n",
      "Epoch 809/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7082 - val_loss: 16.7258\n",
      "Epoch 810/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.7269 - val_loss: 16.2138\n",
      "Epoch 811/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.7229 - val_loss: 16.6669\n",
      "Epoch 812/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7361 - val_loss: 17.3156\n",
      "Epoch 813/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7134 - val_loss: 16.5714\n",
      "Epoch 814/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7392 - val_loss: 16.6623\n",
      "Epoch 815/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7128 - val_loss: 17.0241\n",
      "Epoch 816/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7293 - val_loss: 16.4876\n",
      "Epoch 817/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7193 - val_loss: 15.7345\n",
      "Epoch 818/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7247 - val_loss: 17.5821\n",
      "Epoch 819/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7327 - val_loss: 16.9210\n",
      "Epoch 820/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7101 - val_loss: 16.1938\n",
      "Epoch 821/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7413 - val_loss: 16.3941\n",
      "Epoch 822/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6946 - val_loss: 17.4775\n",
      "Epoch 823/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7253 - val_loss: 16.8093\n",
      "Epoch 824/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7163 - val_loss: 16.6137\n",
      "Epoch 825/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.7055 - val_loss: 17.7096\n",
      "Epoch 826/2000\n",
      "1552/1552 [==============================] - 0s 286us/step - loss: 3.7213 - val_loss: 18.0762\n",
      "Epoch 827/2000\n",
      "1552/1552 [==============================] - 0s 283us/step - loss: 3.7058 - val_loss: 17.2245\n",
      "Epoch 828/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7090 - val_loss: 17.5164\n",
      "Epoch 829/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 3.7268 - val_loss: 17.3311\n",
      "Epoch 830/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 3.7120 - val_loss: 17.5959\n",
      "Epoch 831/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7201 - val_loss: 15.8990\n",
      "Epoch 832/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 3.7093 - val_loss: 16.8170\n",
      "Epoch 833/2000\n",
      "1552/1552 [==============================] - 0s 285us/step - loss: 3.7080 - val_loss: 17.1451\n",
      "Epoch 834/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7046 - val_loss: 16.6594\n",
      "Epoch 835/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.7093 - val_loss: 16.9643\n",
      "Epoch 836/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7316 - val_loss: 16.6523\n",
      "Epoch 837/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7102 - val_loss: 16.8453\n",
      "Epoch 838/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.7077 - val_loss: 16.8828\n",
      "Epoch 839/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7034 - val_loss: 17.9034\n",
      "Epoch 840/2000\n",
      "1552/1552 [==============================] - 0s 293us/step - loss: 3.6963 - val_loss: 16.7730\n",
      "Epoch 841/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7017 - val_loss: 17.0503\n",
      "Epoch 842/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7130 - val_loss: 17.4929\n",
      "Epoch 843/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 3.7095 - val_loss: 17.0778\n",
      "Epoch 844/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.7301 - val_loss: 17.3549\n",
      "Epoch 845/2000\n",
      "1552/1552 [==============================] - 0s 286us/step - loss: 3.6977 - val_loss: 17.7608\n",
      "Epoch 846/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7166 - val_loss: 16.6544\n",
      "Epoch 847/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.7081 - val_loss: 17.0145\n",
      "Epoch 848/2000\n",
      "1552/1552 [==============================] - 0s 290us/step - loss: 3.7035 - val_loss: 16.9732\n",
      "Epoch 849/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7064 - val_loss: 17.5185\n",
      "Epoch 850/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7016 - val_loss: 17.1024\n",
      "Epoch 851/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.7147 - val_loss: 16.4896\n",
      "Epoch 852/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.6924 - val_loss: 16.6794\n",
      "Epoch 853/2000\n",
      "1552/1552 [==============================] - 0s 285us/step - loss: 3.6962 - val_loss: 17.1885\n",
      "Epoch 854/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.6878 - val_loss: 17.5683\n",
      "Epoch 855/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7117 - val_loss: 17.3429\n",
      "Epoch 856/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7326 - val_loss: 17.6215\n",
      "Epoch 857/2000\n",
      "1552/1552 [==============================] - 0s 285us/step - loss: 3.7092 - val_loss: 17.2081\n",
      "Epoch 858/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.7153 - val_loss: 17.0269\n",
      "Epoch 859/2000\n",
      "1552/1552 [==============================] - 0s 286us/step - loss: 3.7084 - val_loss: 17.3107\n",
      "Epoch 860/2000\n",
      "1552/1552 [==============================] - 0s 288us/step - loss: 3.7028 - val_loss: 16.7299\n",
      "Epoch 861/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.6916 - val_loss: 16.3618\n",
      "Epoch 862/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.6893 - val_loss: 16.5348\n",
      "Epoch 863/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.6921 - val_loss: 16.6124\n",
      "Epoch 864/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.7058 - val_loss: 16.5447\n",
      "Epoch 865/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7045 - val_loss: 16.8139\n",
      "Epoch 866/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7048 - val_loss: 16.2285\n",
      "Epoch 867/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7035 - val_loss: 17.0275\n",
      "Epoch 868/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6972 - val_loss: 16.7945\n",
      "Epoch 869/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6876 - val_loss: 16.7287\n",
      "Epoch 870/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.7108 - val_loss: 16.9703\n",
      "Epoch 871/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6905 - val_loss: 16.4074\n",
      "Epoch 872/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.7064 - val_loss: 17.2370\n",
      "Epoch 873/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7045 - val_loss: 16.5939\n",
      "Epoch 874/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6922 - val_loss: 16.4832\n",
      "Epoch 875/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6953 - val_loss: 17.1061\n",
      "Epoch 876/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6897 - val_loss: 17.3648\n",
      "Epoch 877/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7104 - val_loss: 17.6532\n",
      "Epoch 878/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6857 - val_loss: 16.9053\n",
      "Epoch 879/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6962 - val_loss: 17.2686\n",
      "Epoch 880/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6891 - val_loss: 16.7213\n",
      "Epoch 881/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6994 - val_loss: 16.3586\n",
      "Epoch 882/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6796 - val_loss: 16.5399\n",
      "Epoch 883/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6937 - val_loss: 16.9575\n",
      "Epoch 884/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6816 - val_loss: 17.1588\n",
      "Epoch 885/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6908 - val_loss: 17.1849\n",
      "Epoch 886/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7034 - val_loss: 17.4114\n",
      "Epoch 887/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6938 - val_loss: 16.5095\n",
      "Epoch 888/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.7037 - val_loss: 17.0459\n",
      "Epoch 889/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6923 - val_loss: 17.1250\n",
      "Epoch 890/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6852 - val_loss: 16.1113\n",
      "Epoch 891/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6845 - val_loss: 17.1306\n",
      "Epoch 892/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6894 - val_loss: 16.2850\n",
      "Epoch 893/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6991 - val_loss: 17.1767\n",
      "Epoch 894/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6819 - val_loss: 17.3202\n",
      "Epoch 895/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6833 - val_loss: 16.9103\n",
      "Epoch 896/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6825 - val_loss: 16.4493\n",
      "Epoch 897/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6893 - val_loss: 16.6764\n",
      "Epoch 898/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6971 - val_loss: 16.3384\n",
      "Epoch 899/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6813 - val_loss: 16.6784\n",
      "Epoch 900/2000\n",
      "1552/1552 [==============================] - 43s 28ms/step - loss: 3.6922 - val_loss: 16.9780\n",
      "Epoch 901/2000\n",
      "1552/1552 [==============================] - 0s 289us/step - loss: 3.6797 - val_loss: 16.4281\n",
      "Epoch 902/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6795 - val_loss: 17.4705\n",
      "Epoch 903/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6980 - val_loss: 17.1325\n",
      "Epoch 904/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6838 - val_loss: 17.0382\n",
      "Epoch 905/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6798 - val_loss: 17.4327\n",
      "Epoch 906/2000\n",
      "1552/1552 [==============================] - 0s 286us/step - loss: 3.6945 - val_loss: 16.7762\n",
      "Epoch 907/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6807 - val_loss: 17.5121\n",
      "Epoch 908/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6952 - val_loss: 17.4495\n",
      "Epoch 909/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6901 - val_loss: 17.0949\n",
      "Epoch 910/2000\n",
      "1552/1552 [==============================] - 1s 354us/step - loss: 3.6753 - val_loss: 16.5887\n",
      "Epoch 911/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.6835 - val_loss: 17.5985\n",
      "Epoch 912/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.7026 - val_loss: 16.9246\n",
      "Epoch 913/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6841 - val_loss: 17.2113\n",
      "Epoch 914/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.6868 - val_loss: 16.6679\n",
      "Epoch 915/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6903 - val_loss: 16.1647\n",
      "Epoch 916/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.7126 - val_loss: 17.3804\n",
      "Epoch 917/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.6749 - val_loss: 16.6681\n",
      "Epoch 918/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6843 - val_loss: 17.1241\n",
      "Epoch 919/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6910 - val_loss: 17.7932\n",
      "Epoch 920/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6875 - val_loss: 17.6735\n",
      "Epoch 921/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6893 - val_loss: 17.3629\n",
      "Epoch 922/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6893 - val_loss: 16.5721\n",
      "Epoch 923/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6895 - val_loss: 16.6236\n",
      "Epoch 924/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6770 - val_loss: 16.8413\n",
      "Epoch 925/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6733 - val_loss: 16.2942\n",
      "Epoch 926/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6920 - val_loss: 17.1194\n",
      "Epoch 927/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6746 - val_loss: 17.6518\n",
      "Epoch 928/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6919 - val_loss: 17.4925\n",
      "Epoch 929/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6740 - val_loss: 17.6601\n",
      "Epoch 930/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6757 - val_loss: 16.7121\n",
      "Epoch 931/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6930 - val_loss: 16.9429\n",
      "Epoch 932/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6697 - val_loss: 17.4044\n",
      "Epoch 933/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6816 - val_loss: 17.7465\n",
      "Epoch 934/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6792 - val_loss: 18.4721\n",
      "Epoch 935/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6873 - val_loss: 17.0208\n",
      "Epoch 936/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6856 - val_loss: 17.5668\n",
      "Epoch 937/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6837 - val_loss: 16.2440\n",
      "Epoch 938/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6847 - val_loss: 17.2560\n",
      "Epoch 939/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6704 - val_loss: 17.4121\n",
      "Epoch 940/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6712 - val_loss: 17.9196\n",
      "Epoch 941/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6834 - val_loss: 17.1525\n",
      "Epoch 942/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6956 - val_loss: 17.9147\n",
      "Epoch 943/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6730 - val_loss: 16.8735\n",
      "Epoch 944/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6566 - val_loss: 16.7468\n",
      "Epoch 945/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6836 - val_loss: 17.0921\n",
      "Epoch 946/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6546 - val_loss: 17.0154\n",
      "Epoch 947/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6967 - val_loss: 17.6655\n",
      "Epoch 948/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6802 - val_loss: 17.3440\n",
      "Epoch 949/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6691 - val_loss: 16.9425\n",
      "Epoch 950/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6738 - val_loss: 17.5803\n",
      "Epoch 951/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.6584 - val_loss: 17.3197\n",
      "Epoch 952/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6997 - val_loss: 16.7739\n",
      "Epoch 953/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6625 - val_loss: 16.5291\n",
      "Epoch 954/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6724 - val_loss: 17.0707\n",
      "Epoch 955/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6810 - val_loss: 18.1821\n",
      "Epoch 956/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6789 - val_loss: 17.4346\n",
      "Epoch 957/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6907 - val_loss: 16.6786\n",
      "Epoch 958/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6570 - val_loss: 17.0619\n",
      "Epoch 959/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.7028 - val_loss: 16.4272\n",
      "Epoch 960/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6745 - val_loss: 17.4623\n",
      "Epoch 961/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6676 - val_loss: 17.1958\n",
      "Epoch 962/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6876 - val_loss: 16.7103\n",
      "Epoch 963/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6754 - val_loss: 17.5426\n",
      "Epoch 964/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6813 - val_loss: 16.3726\n",
      "Epoch 965/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6662 - val_loss: 16.3664\n",
      "Epoch 966/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6871 - val_loss: 16.7702\n",
      "Epoch 967/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6693 - val_loss: 18.0893\n",
      "Epoch 968/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6887 - val_loss: 17.0469\n",
      "Epoch 969/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6580 - val_loss: 17.7950\n",
      "Epoch 970/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6672 - val_loss: 17.2768\n",
      "Epoch 971/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6715 - val_loss: 17.2532\n",
      "Epoch 972/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6778 - val_loss: 17.6104\n",
      "Epoch 973/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6676 - val_loss: 16.9356\n",
      "Epoch 974/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6776 - val_loss: 17.0061\n",
      "Epoch 975/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6845 - val_loss: 18.2421\n",
      "Epoch 976/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6642 - val_loss: 17.7513\n",
      "Epoch 977/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6727 - val_loss: 17.3611\n",
      "Epoch 978/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6696 - val_loss: 17.2087\n",
      "Epoch 979/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6786 - val_loss: 16.9197\n",
      "Epoch 980/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6716 - val_loss: 17.0377\n",
      "Epoch 981/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6749 - val_loss: 16.9252\n",
      "Epoch 982/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6738 - val_loss: 18.0561\n",
      "Epoch 983/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6613 - val_loss: 17.5316\n",
      "Epoch 984/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6713 - val_loss: 17.6143\n",
      "Epoch 985/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6776 - val_loss: 17.0523\n",
      "Epoch 986/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6812 - val_loss: 16.8990\n",
      "Epoch 987/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6609 - val_loss: 17.1764\n",
      "Epoch 988/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6698 - val_loss: 17.4205\n",
      "Epoch 989/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6661 - val_loss: 16.8541\n",
      "Epoch 990/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6773 - val_loss: 17.1383\n",
      "Epoch 991/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6583 - val_loss: 17.4540\n",
      "Epoch 992/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6621 - val_loss: 17.2594\n",
      "Epoch 993/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6710 - val_loss: 17.3757\n",
      "Epoch 994/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6898 - val_loss: 18.4774\n",
      "Epoch 995/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6528 - val_loss: 16.7816\n",
      "Epoch 996/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6646 - val_loss: 17.4800\n",
      "Epoch 997/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6750 - val_loss: 17.3614\n",
      "Epoch 998/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6687 - val_loss: 18.0363\n",
      "Epoch 999/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6726 - val_loss: 17.5515\n",
      "Epoch 1000/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6630 - val_loss: 17.1615\n",
      "Epoch 1001/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6512 - val_loss: 16.8727\n",
      "Epoch 1002/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6762 - val_loss: 16.6761\n",
      "Epoch 1003/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6689 - val_loss: 17.0857\n",
      "Epoch 1004/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6526 - val_loss: 17.4243\n",
      "Epoch 1005/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6511 - val_loss: 17.2624\n",
      "Epoch 1006/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6688 - val_loss: 17.9300\n",
      "Epoch 1007/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6671 - val_loss: 17.1013\n",
      "Epoch 1008/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6565 - val_loss: 17.4131\n",
      "Epoch 1009/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6502 - val_loss: 17.2544\n",
      "Epoch 1010/2000\n",
      "1552/1552 [==============================] - 48s 31ms/step - loss: 3.6629 - val_loss: 16.9273\n",
      "Epoch 1011/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.6596 - val_loss: 17.2718\n",
      "Epoch 1012/2000\n",
      "1552/1552 [==============================] - 0s 287us/step - loss: 3.6524 - val_loss: 17.2270\n",
      "Epoch 1013/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6554 - val_loss: 17.4301\n",
      "Epoch 1014/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6495 - val_loss: 17.3926\n",
      "Epoch 1015/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6481 - val_loss: 18.4062\n",
      "Epoch 1016/2000\n",
      "1552/1552 [==============================] - 0s 310us/step - loss: 3.6420 - val_loss: 17.8260\n",
      "Epoch 1017/2000\n",
      "1552/1552 [==============================] - 0s 290us/step - loss: 3.6566 - val_loss: 17.1485\n",
      "Epoch 1018/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6663 - val_loss: 17.2524\n",
      "Epoch 1019/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6528 - val_loss: 16.8313\n",
      "Epoch 1020/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6571 - val_loss: 17.9773\n",
      "Epoch 1021/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6532 - val_loss: 17.1696\n",
      "Epoch 1022/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6797 - val_loss: 17.6663\n",
      "Epoch 1023/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6607 - val_loss: 17.6652\n",
      "Epoch 1024/2000\n",
      "1552/1552 [==============================] - 0s 321us/step - loss: 3.6707 - val_loss: 17.5597\n",
      "Epoch 1025/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6686 - val_loss: 17.8097\n",
      "Epoch 1026/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6710 - val_loss: 17.4799\n",
      "Epoch 1027/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6560 - val_loss: 17.5145\n",
      "Epoch 1028/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6557 - val_loss: 17.6198\n",
      "Epoch 1029/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6466 - val_loss: 16.8341\n",
      "Epoch 1030/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6432 - val_loss: 17.1024\n",
      "Epoch 1031/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6640 - val_loss: 17.6731\n",
      "Epoch 1032/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6496 - val_loss: 16.9367\n",
      "Epoch 1033/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6455 - val_loss: 17.4062\n",
      "Epoch 1034/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6551 - val_loss: 17.3903\n",
      "Epoch 1035/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6582 - val_loss: 18.4002\n",
      "Epoch 1036/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6666 - val_loss: 17.0344\n",
      "Epoch 1037/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6601 - val_loss: 17.7159\n",
      "Epoch 1038/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6509 - val_loss: 16.7995\n",
      "Epoch 1039/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6677 - val_loss: 17.3424\n",
      "Epoch 1040/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6735 - val_loss: 17.1025\n",
      "Epoch 1041/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6514 - val_loss: 18.5263\n",
      "Epoch 1042/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6471 - val_loss: 17.5320\n",
      "Epoch 1043/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6603 - val_loss: 16.7402\n",
      "Epoch 1044/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6697 - val_loss: 17.0958\n",
      "Epoch 1045/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6570 - val_loss: 16.9055\n",
      "Epoch 1046/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6438 - val_loss: 17.1797\n",
      "Epoch 1047/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6541 - val_loss: 18.2787\n",
      "Epoch 1048/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6560 - val_loss: 17.1447\n",
      "Epoch 1049/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6347 - val_loss: 17.9084\n",
      "Epoch 1050/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6638 - val_loss: 18.0871\n",
      "Epoch 1051/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6633 - val_loss: 17.0740\n",
      "Epoch 1052/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6469 - val_loss: 17.7097\n",
      "Epoch 1053/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6414 - val_loss: 16.8314\n",
      "Epoch 1054/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6566 - val_loss: 17.5865\n",
      "Epoch 1055/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6534 - val_loss: 18.0492\n",
      "Epoch 1056/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6643 - val_loss: 18.0980\n",
      "Epoch 1057/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6449 - val_loss: 16.6975\n",
      "Epoch 1058/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6517 - val_loss: 17.1502\n",
      "Epoch 1059/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6510 - val_loss: 17.6084\n",
      "Epoch 1060/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6505 - val_loss: 17.5624\n",
      "Epoch 1061/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6493 - val_loss: 17.6659\n",
      "Epoch 1062/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6643 - val_loss: 17.2129\n",
      "Epoch 1063/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6525 - val_loss: 18.0640\n",
      "Epoch 1064/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6669 - val_loss: 17.4230\n",
      "Epoch 1065/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6636 - val_loss: 17.8149\n",
      "Epoch 1066/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6501 - val_loss: 17.4663\n",
      "Epoch 1067/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6443 - val_loss: 17.1382\n",
      "Epoch 1068/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6530 - val_loss: 17.4299\n",
      "Epoch 1069/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6524 - val_loss: 19.3928\n",
      "Epoch 1070/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6491 - val_loss: 17.7235\n",
      "Epoch 1071/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6454 - val_loss: 17.1044\n",
      "Epoch 1072/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6448 - val_loss: 18.3040\n",
      "Epoch 1073/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6807 - val_loss: 18.0679\n",
      "Epoch 1074/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6470 - val_loss: 17.1366\n",
      "Epoch 1075/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6462 - val_loss: 18.7195\n",
      "Epoch 1076/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6380 - val_loss: 17.5800\n",
      "Epoch 1077/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6479 - val_loss: 17.8443\n",
      "Epoch 1078/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6375 - val_loss: 17.6640\n",
      "Epoch 1079/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6599 - val_loss: 18.0661\n",
      "Epoch 1080/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6458 - val_loss: 17.6061\n",
      "Epoch 1081/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6522 - val_loss: 17.8098\n",
      "Epoch 1082/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6570 - val_loss: 18.1586\n",
      "Epoch 1083/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6378 - val_loss: 17.4869\n",
      "Epoch 1084/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6383 - val_loss: 17.4546\n",
      "Epoch 1085/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6513 - val_loss: 16.6493\n",
      "Epoch 1086/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6314 - val_loss: 18.0171\n",
      "Epoch 1087/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6273 - val_loss: 17.5280\n",
      "Epoch 1088/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6432 - val_loss: 16.7969\n",
      "Epoch 1089/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6435 - val_loss: 17.8937\n",
      "Epoch 1090/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6394 - val_loss: 17.2879\n",
      "Epoch 1091/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6522 - val_loss: 18.3629\n",
      "Epoch 1092/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6423 - val_loss: 17.7923\n",
      "Epoch 1093/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6535 - val_loss: 18.0374\n",
      "Epoch 1094/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6401 - val_loss: 17.2106\n",
      "Epoch 1095/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6512 - val_loss: 17.3189\n",
      "Epoch 1096/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6311 - val_loss: 17.4830\n",
      "Epoch 1097/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6343 - val_loss: 17.8282\n",
      "Epoch 1098/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6490 - val_loss: 17.8771\n",
      "Epoch 1099/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6454 - val_loss: 17.8412\n",
      "Epoch 1100/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6436 - val_loss: 17.6322\n",
      "Epoch 1101/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6461 - val_loss: 18.0119\n",
      "Epoch 1102/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6388 - val_loss: 17.8360\n",
      "Epoch 1103/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6669 - val_loss: 17.1028\n",
      "Epoch 1104/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6312 - val_loss: 17.1299\n",
      "Epoch 1105/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6472 - val_loss: 17.2101\n",
      "Epoch 1106/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6253 - val_loss: 17.5398\n",
      "Epoch 1107/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6368 - val_loss: 18.7595\n",
      "Epoch 1108/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6445 - val_loss: 17.4074\n",
      "Epoch 1109/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6629 - val_loss: 17.3040\n",
      "Epoch 1110/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6395 - val_loss: 18.0947\n",
      "Epoch 1111/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6342 - val_loss: 17.1224\n",
      "Epoch 1112/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6419 - val_loss: 16.9277\n",
      "Epoch 1113/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6444 - val_loss: 17.3655\n",
      "Epoch 1114/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6442 - val_loss: 18.1806\n",
      "Epoch 1115/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6469 - val_loss: 17.2693\n",
      "Epoch 1116/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6226 - val_loss: 18.1033\n",
      "Epoch 1117/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6247 - val_loss: 17.5965\n",
      "Epoch 1118/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6553 - val_loss: 17.4636\n",
      "Epoch 1119/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6327 - val_loss: 18.1201\n",
      "Epoch 1120/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6385 - val_loss: 17.2257\n",
      "Epoch 1121/2000\n",
      "1552/1552 [==============================] - 43s 28ms/step - loss: 3.6291 - val_loss: 18.3232\n",
      "Epoch 1122/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6563 - val_loss: 19.0424\n",
      "Epoch 1123/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6371 - val_loss: 16.6018\n",
      "Epoch 1124/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6517 - val_loss: 17.0245\n",
      "Epoch 1125/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6263 - val_loss: 18.2431\n",
      "Epoch 1126/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6446 - val_loss: 18.1897\n",
      "Epoch 1127/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6499 - val_loss: 17.5937\n",
      "Epoch 1128/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6475 - val_loss: 17.9135\n",
      "Epoch 1129/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6337 - val_loss: 18.1057\n",
      "Epoch 1130/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6386 - val_loss: 17.9596\n",
      "Epoch 1131/2000\n",
      "1552/1552 [==============================] - 0s 321us/step - loss: 3.6306 - val_loss: 17.0857\n",
      "Epoch 1132/2000\n",
      "1552/1552 [==============================] - 0s 283us/step - loss: 3.6336 - val_loss: 16.7337\n",
      "Epoch 1133/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.6413 - val_loss: 17.1743\n",
      "Epoch 1134/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6224 - val_loss: 16.9185\n",
      "Epoch 1135/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.6359 - val_loss: 17.3373\n",
      "Epoch 1136/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6532 - val_loss: 18.4624\n",
      "Epoch 1137/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6361 - val_loss: 17.5184\n",
      "Epoch 1138/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6464 - val_loss: 17.9344\n",
      "Epoch 1139/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6297 - val_loss: 18.5950\n",
      "Epoch 1140/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6214 - val_loss: 17.2297\n",
      "Epoch 1141/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6469 - val_loss: 17.0005\n",
      "Epoch 1142/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6475 - val_loss: 18.2984\n",
      "Epoch 1143/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6218 - val_loss: 17.8266\n",
      "Epoch 1144/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.6437 - val_loss: 18.2533\n",
      "Epoch 1145/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6291 - val_loss: 17.9379\n",
      "Epoch 1146/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6415 - val_loss: 17.4441\n",
      "Epoch 1147/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6476 - val_loss: 18.3793\n",
      "Epoch 1148/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6355 - val_loss: 17.5875\n",
      "Epoch 1149/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6351 - val_loss: 17.6735\n",
      "Epoch 1150/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6372 - val_loss: 17.6411\n",
      "Epoch 1151/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6266 - val_loss: 17.7508\n",
      "Epoch 1152/2000\n",
      "1552/1552 [==============================] - 0s 293us/step - loss: 3.6526 - val_loss: 18.2375\n",
      "Epoch 1153/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6372 - val_loss: 17.8141\n",
      "Epoch 1154/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6340 - val_loss: 17.6186\n",
      "Epoch 1155/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6593 - val_loss: 18.1194\n",
      "Epoch 1156/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6416 - val_loss: 17.1214\n",
      "Epoch 1157/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6187 - val_loss: 17.7757\n",
      "Epoch 1158/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6380 - val_loss: 18.2220\n",
      "Epoch 1159/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6316 - val_loss: 17.3992\n",
      "Epoch 1160/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6356 - val_loss: 17.5650\n",
      "Epoch 1161/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6248 - val_loss: 17.6695\n",
      "Epoch 1162/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6488 - val_loss: 18.6388\n",
      "Epoch 1163/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6303 - val_loss: 18.5595\n",
      "Epoch 1164/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6211 - val_loss: 17.3377\n",
      "Epoch 1165/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6291 - val_loss: 17.3293\n",
      "Epoch 1166/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6346 - val_loss: 17.5373\n",
      "Epoch 1167/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6588 - val_loss: 16.9052\n",
      "Epoch 1168/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6370 - val_loss: 17.7703\n",
      "Epoch 1169/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6374 - val_loss: 17.4327\n",
      "Epoch 1170/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6216 - val_loss: 17.7109\n",
      "Epoch 1171/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6539 - val_loss: 17.5075\n",
      "Epoch 1172/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6294 - val_loss: 18.0525\n",
      "Epoch 1173/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6536 - val_loss: 17.7925\n",
      "Epoch 1174/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6270 - val_loss: 18.0182\n",
      "Epoch 1175/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6281 - val_loss: 17.5627\n",
      "Epoch 1176/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6268 - val_loss: 18.1293\n",
      "Epoch 1177/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6344 - val_loss: 18.1962\n",
      "Epoch 1178/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6284 - val_loss: 17.5044\n",
      "Epoch 1179/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6242 - val_loss: 16.9763\n",
      "Epoch 1180/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6391 - val_loss: 17.1153\n",
      "Epoch 1181/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6499 - val_loss: 17.3147\n",
      "Epoch 1182/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6361 - val_loss: 17.6990\n",
      "Epoch 1183/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6254 - val_loss: 17.2729\n",
      "Epoch 1184/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6208 - val_loss: 17.3709\n",
      "Epoch 1185/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6598 - val_loss: 17.0280\n",
      "Epoch 1186/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6236 - val_loss: 17.5405\n",
      "Epoch 1187/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6300 - val_loss: 17.8065\n",
      "Epoch 1188/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6239 - val_loss: 17.4336\n",
      "Epoch 1189/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6245 - val_loss: 17.3072\n",
      "Epoch 1190/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6284 - val_loss: 17.1016\n",
      "Epoch 1191/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6505 - val_loss: 17.7160\n",
      "Epoch 1192/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6238 - val_loss: 18.2137\n",
      "Epoch 1193/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6389 - val_loss: 17.5754\n",
      "Epoch 1194/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6202 - val_loss: 18.2605\n",
      "Epoch 1195/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6416 - val_loss: 18.3289\n",
      "Epoch 1196/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6150 - val_loss: 16.8644\n",
      "Epoch 1197/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6253 - val_loss: 17.1469\n",
      "Epoch 1198/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6273 - val_loss: 17.2018\n",
      "Epoch 1199/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6114 - val_loss: 18.6144\n",
      "Epoch 1200/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6173 - val_loss: 18.0010\n",
      "Epoch 1201/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6341 - val_loss: 17.9069\n",
      "Epoch 1202/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6428 - val_loss: 18.1824\n",
      "Epoch 1203/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6320 - val_loss: 17.5725\n",
      "Epoch 1204/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6441 - val_loss: 17.9855\n",
      "Epoch 1205/2000\n",
      "1552/1552 [==============================] - 0s 283us/step - loss: 3.6343 - val_loss: 17.7064\n",
      "Epoch 1206/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6217 - val_loss: 17.5431\n",
      "Epoch 1207/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6205 - val_loss: 17.7793\n",
      "Epoch 1208/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6300 - val_loss: 17.0357\n",
      "Epoch 1209/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6345 - val_loss: 16.8831\n",
      "Epoch 1210/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6262 - val_loss: 17.1321\n",
      "Epoch 1211/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6241 - val_loss: 19.0744\n",
      "Epoch 1212/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6173 - val_loss: 18.6594\n",
      "Epoch 1213/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6285 - val_loss: 17.3084\n",
      "Epoch 1214/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6062 - val_loss: 17.6051\n",
      "Epoch 1215/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6228 - val_loss: 17.8397\n",
      "Epoch 1216/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6319 - val_loss: 17.2021\n",
      "Epoch 1217/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6270 - val_loss: 17.2795\n",
      "Epoch 1218/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6189 - val_loss: 17.0114\n",
      "Epoch 1219/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6182 - val_loss: 17.4054\n",
      "Epoch 1220/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6390 - val_loss: 17.6961\n",
      "Epoch 1221/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6303 - val_loss: 18.8510\n",
      "Epoch 1222/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6265 - val_loss: 18.3396\n",
      "Epoch 1223/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6087 - val_loss: 17.8200\n",
      "Epoch 1224/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6287 - val_loss: 18.8049\n",
      "Epoch 1225/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6276 - val_loss: 17.3769\n",
      "Epoch 1226/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6247 - val_loss: 17.6812\n",
      "Epoch 1227/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6217 - val_loss: 18.2376\n",
      "Epoch 1228/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6282 - val_loss: 16.8297\n",
      "Epoch 1229/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6181 - val_loss: 17.8044\n",
      "Epoch 1230/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6299 - val_loss: 17.1097\n",
      "Epoch 1231/2000\n",
      "1552/1552 [==============================] - 54s 35ms/step - loss: 3.6130 - val_loss: 17.6239\n",
      "Epoch 1232/2000\n",
      "1552/1552 [==============================] - 0s 316us/step - loss: 3.6204 - val_loss: 17.9298\n",
      "Epoch 1233/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.6154 - val_loss: 18.2135\n",
      "Epoch 1234/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6357 - val_loss: 17.2377\n",
      "Epoch 1235/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6088 - val_loss: 17.8997\n",
      "Epoch 1236/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6302 - val_loss: 17.6910\n",
      "Epoch 1237/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.6257 - val_loss: 17.8859\n",
      "Epoch 1238/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6165 - val_loss: 17.9075\n",
      "Epoch 1239/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6208 - val_loss: 17.6341\n",
      "Epoch 1240/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6350 - val_loss: 18.3489\n",
      "Epoch 1241/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6181 - val_loss: 18.2702\n",
      "Epoch 1242/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6265 - val_loss: 17.5010\n",
      "Epoch 1243/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6185 - val_loss: 17.6087\n",
      "Epoch 1244/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6062 - val_loss: 16.8641\n",
      "Epoch 1245/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6194 - val_loss: 17.3720\n",
      "Epoch 1246/2000\n",
      "1552/1552 [==============================] - 1s 324us/step - loss: 3.6367 - val_loss: 17.6095\n",
      "Epoch 1247/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6186 - val_loss: 17.1879\n",
      "Epoch 1248/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6286 - val_loss: 17.0930\n",
      "Epoch 1249/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6271 - val_loss: 17.8137\n",
      "Epoch 1250/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6255 - val_loss: 18.7771\n",
      "Epoch 1251/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6169 - val_loss: 17.8018\n",
      "Epoch 1252/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6147 - val_loss: 17.5500\n",
      "Epoch 1253/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6265 - val_loss: 17.6128\n",
      "Epoch 1254/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6144 - val_loss: 18.1087\n",
      "Epoch 1255/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6153 - val_loss: 19.1126\n",
      "Epoch 1256/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.6147 - val_loss: 17.3884\n",
      "Epoch 1257/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6209 - val_loss: 17.5986\n",
      "Epoch 1258/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6232 - val_loss: 18.5903\n",
      "Epoch 1259/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6206 - val_loss: 18.8700\n",
      "Epoch 1260/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6183 - val_loss: 18.0797\n",
      "Epoch 1261/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6242 - val_loss: 17.9224\n",
      "Epoch 1262/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6147 - val_loss: 18.1389\n",
      "Epoch 1263/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6049 - val_loss: 17.3942\n",
      "Epoch 1264/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6211 - val_loss: 17.5614\n",
      "Epoch 1265/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6264 - val_loss: 17.4104\n",
      "Epoch 1266/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6151 - val_loss: 16.9727\n",
      "Epoch 1267/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6147 - val_loss: 17.6965\n",
      "Epoch 1268/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6250 - val_loss: 17.1690\n",
      "Epoch 1269/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6166 - val_loss: 17.3692\n",
      "Epoch 1270/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6253 - val_loss: 17.1048\n",
      "Epoch 1271/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6306 - val_loss: 17.5713\n",
      "Epoch 1272/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6217 - val_loss: 18.0289\n",
      "Epoch 1273/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6278 - val_loss: 17.1884\n",
      "Epoch 1274/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6099 - val_loss: 17.3605\n",
      "Epoch 1275/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6125 - val_loss: 17.1490\n",
      "Epoch 1276/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6067 - val_loss: 18.6263\n",
      "Epoch 1277/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6291 - val_loss: 17.3581\n",
      "Epoch 1278/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6172 - val_loss: 18.0779\n",
      "Epoch 1279/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6394 - val_loss: 17.3206\n",
      "Epoch 1280/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6053 - val_loss: 17.9895\n",
      "Epoch 1281/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6229 - val_loss: 18.3829\n",
      "Epoch 1282/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.6049 - val_loss: 17.6907\n",
      "Epoch 1283/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6203 - val_loss: 18.3023\n",
      "Epoch 1284/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6213 - val_loss: 17.5348\n",
      "Epoch 1285/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6255 - val_loss: 17.6118\n",
      "Epoch 1286/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6285 - val_loss: 17.6301\n",
      "Epoch 1287/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6257 - val_loss: 17.8514\n",
      "Epoch 1288/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6051 - val_loss: 17.8308\n",
      "Epoch 1289/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6108 - val_loss: 18.0145\n",
      "Epoch 1290/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6247 - val_loss: 17.9571\n",
      "Epoch 1291/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6163 - val_loss: 17.3834\n",
      "Epoch 1292/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6175 - val_loss: 18.3587\n",
      "Epoch 1293/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6305 - val_loss: 17.4290\n",
      "Epoch 1294/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6193 - val_loss: 18.2235\n",
      "Epoch 1295/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6118 - val_loss: 17.8899\n",
      "Epoch 1296/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6127 - val_loss: 17.1732\n",
      "Epoch 1297/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6062 - val_loss: 18.6264\n",
      "Epoch 1298/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6173 - val_loss: 17.8589\n",
      "Epoch 1299/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5988 - val_loss: 17.3417\n",
      "Epoch 1300/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6200 - val_loss: 17.6394\n",
      "Epoch 1301/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6165 - val_loss: 18.2685\n",
      "Epoch 1302/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6141 - val_loss: 18.0483\n",
      "Epoch 1303/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6188 - val_loss: 18.2721\n",
      "Epoch 1304/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6243 - val_loss: 17.7878\n",
      "Epoch 1305/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6062 - val_loss: 17.7235\n",
      "Epoch 1306/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6181 - val_loss: 17.0945\n",
      "Epoch 1307/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6040 - val_loss: 17.9958\n",
      "Epoch 1308/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6070 - val_loss: 18.6381\n",
      "Epoch 1309/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5976 - val_loss: 19.4914\n",
      "Epoch 1310/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6193 - val_loss: 18.2119\n",
      "Epoch 1311/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6116 - val_loss: 17.7665\n",
      "Epoch 1312/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6093 - val_loss: 18.1455\n",
      "Epoch 1313/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6157 - val_loss: 17.1690\n",
      "Epoch 1314/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6220 - val_loss: 17.8476\n",
      "Epoch 1315/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6181 - val_loss: 18.2253\n",
      "Epoch 1316/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6091 - val_loss: 18.1980\n",
      "Epoch 1317/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6115 - val_loss: 18.0839\n",
      "Epoch 1318/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.6052 - val_loss: 18.6009\n",
      "Epoch 1319/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6135 - val_loss: 17.5749\n",
      "Epoch 1320/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6244 - val_loss: 17.4776\n",
      "Epoch 1321/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5965 - val_loss: 18.1406\n",
      "Epoch 1322/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6194 - val_loss: 18.2533\n",
      "Epoch 1323/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6160 - val_loss: 17.7665\n",
      "Epoch 1324/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6238 - val_loss: 18.4098\n",
      "Epoch 1325/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6230 - val_loss: 17.3774\n",
      "Epoch 1326/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6119 - val_loss: 17.3268\n",
      "Epoch 1327/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6262 - val_loss: 18.3869\n",
      "Epoch 1328/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6086 - val_loss: 17.5219\n",
      "Epoch 1329/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6019 - val_loss: 17.7826\n",
      "Epoch 1330/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6104 - val_loss: 17.1996\n",
      "Epoch 1331/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6050 - val_loss: 17.2683\n",
      "Epoch 1332/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6292 - val_loss: 17.0226\n",
      "Epoch 1333/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6011 - val_loss: 17.4921\n",
      "Epoch 1334/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6034 - val_loss: 17.5284\n",
      "Epoch 1335/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6048 - val_loss: 17.8727\n",
      "Epoch 1336/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6098 - val_loss: 17.1197\n",
      "Epoch 1337/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6042 - val_loss: 17.6369\n",
      "Epoch 1338/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.6183 - val_loss: 18.1478\n",
      "Epoch 1339/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6084 - val_loss: 18.0656\n",
      "Epoch 1340/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5962 - val_loss: 18.6401\n",
      "Epoch 1341/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6048 - val_loss: 18.1510\n",
      "Epoch 1342/2000\n",
      "1552/1552 [==============================] - 48s 31ms/step - loss: 3.6120 - val_loss: 16.8379\n",
      "Epoch 1343/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5999 - val_loss: 17.8695\n",
      "Epoch 1344/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.6198 - val_loss: 17.8113\n",
      "Epoch 1345/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6173 - val_loss: 17.5975\n",
      "Epoch 1346/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6234 - val_loss: 17.5261\n",
      "Epoch 1347/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6036 - val_loss: 20.0413\n",
      "Epoch 1348/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.6042 - val_loss: 17.8981\n",
      "Epoch 1349/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5990 - val_loss: 18.1108\n",
      "Epoch 1350/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6110 - val_loss: 18.0392\n",
      "Epoch 1351/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6212 - val_loss: 17.5637\n",
      "Epoch 1352/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6090 - val_loss: 18.2591\n",
      "Epoch 1353/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6062 - val_loss: 17.6757\n",
      "Epoch 1354/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6192 - val_loss: 18.3904\n",
      "Epoch 1355/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6047 - val_loss: 18.7583\n",
      "Epoch 1356/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.6044 - val_loss: 18.2692\n",
      "Epoch 1357/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6070 - val_loss: 17.2772\n",
      "Epoch 1358/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5980 - val_loss: 17.8241\n",
      "Epoch 1359/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5983 - val_loss: 17.6825\n",
      "Epoch 1360/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6082 - val_loss: 18.3351\n",
      "Epoch 1361/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5991 - val_loss: 17.1106\n",
      "Epoch 1362/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6023 - val_loss: 17.3782\n",
      "Epoch 1363/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6115 - val_loss: 18.2712\n",
      "Epoch 1364/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5973 - val_loss: 18.4208\n",
      "Epoch 1365/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6047 - val_loss: 16.5600\n",
      "Epoch 1366/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6111 - val_loss: 17.1060\n",
      "Epoch 1367/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6134 - val_loss: 17.8363\n",
      "Epoch 1368/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6078 - val_loss: 18.2013\n",
      "Epoch 1369/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6044 - val_loss: 18.4796\n",
      "Epoch 1370/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.6025 - val_loss: 17.3407\n",
      "Epoch 1371/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5995 - val_loss: 17.8123\n",
      "Epoch 1372/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6030 - val_loss: 17.4188\n",
      "Epoch 1373/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6094 - val_loss: 17.3148\n",
      "Epoch 1374/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6032 - val_loss: 18.6921\n",
      "Epoch 1375/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6042 - val_loss: 18.1619\n",
      "Epoch 1376/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5963 - val_loss: 17.8344\n",
      "Epoch 1377/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6119 - val_loss: 17.5561\n",
      "Epoch 1378/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6040 - val_loss: 17.5430\n",
      "Epoch 1379/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6020 - val_loss: 18.1850\n",
      "Epoch 1380/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6006 - val_loss: 17.3073\n",
      "Epoch 1381/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6012 - val_loss: 17.8821\n",
      "Epoch 1382/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6206 - val_loss: 17.5511\n",
      "Epoch 1383/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.6072 - val_loss: 17.5244\n",
      "Epoch 1384/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6059 - val_loss: 17.8183\n",
      "Epoch 1385/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6036 - val_loss: 18.2020\n",
      "Epoch 1386/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6158 - val_loss: 17.6488\n",
      "Epoch 1387/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5964 - val_loss: 17.8612\n",
      "Epoch 1388/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6102 - val_loss: 18.2044\n",
      "Epoch 1389/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6005 - val_loss: 17.7113\n",
      "Epoch 1390/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5956 - val_loss: 17.3796\n",
      "Epoch 1391/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5970 - val_loss: 18.4830\n",
      "Epoch 1392/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6099 - val_loss: 18.0945\n",
      "Epoch 1393/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.6053 - val_loss: 17.9073\n",
      "Epoch 1394/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6045 - val_loss: 18.7691\n",
      "Epoch 1395/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6032 - val_loss: 17.2208\n",
      "Epoch 1396/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6040 - val_loss: 17.8380\n",
      "Epoch 1397/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6132 - val_loss: 18.2410\n",
      "Epoch 1398/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6154 - val_loss: 17.8754\n",
      "Epoch 1399/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6020 - val_loss: 17.1968\n",
      "Epoch 1400/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5933 - val_loss: 17.5236\n",
      "Epoch 1401/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5975 - val_loss: 17.2519\n",
      "Epoch 1402/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5864 - val_loss: 17.8932\n",
      "Epoch 1403/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6205 - val_loss: 17.3794\n",
      "Epoch 1404/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5854 - val_loss: 17.5950\n",
      "Epoch 1405/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6043 - val_loss: 17.6211\n",
      "Epoch 1406/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5967 - val_loss: 18.0673\n",
      "Epoch 1407/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6117 - val_loss: 17.5446\n",
      "Epoch 1408/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6043 - val_loss: 17.5701\n",
      "Epoch 1409/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5872 - val_loss: 17.7055\n",
      "Epoch 1410/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5979 - val_loss: 18.6082\n",
      "Epoch 1411/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6240 - val_loss: 18.1193\n",
      "Epoch 1412/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6163 - val_loss: 17.9012\n",
      "Epoch 1413/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5657 - val_loss: 18.0100\n",
      "Epoch 1414/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6049 - val_loss: 17.3689\n",
      "Epoch 1415/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6041 - val_loss: 17.8775\n",
      "Epoch 1416/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5896 - val_loss: 18.3482\n",
      "Epoch 1417/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5876 - val_loss: 18.2116\n",
      "Epoch 1418/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6073 - val_loss: 18.4773\n",
      "Epoch 1419/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5949 - val_loss: 17.7274\n",
      "Epoch 1420/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5935 - val_loss: 17.9453\n",
      "Epoch 1421/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5881 - val_loss: 17.2899\n",
      "Epoch 1422/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6120 - val_loss: 17.2837\n",
      "Epoch 1423/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5848 - val_loss: 17.6585\n",
      "Epoch 1424/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5922 - val_loss: 18.0005\n",
      "Epoch 1425/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6063 - val_loss: 18.5435\n",
      "Epoch 1426/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5837 - val_loss: 17.9127\n",
      "Epoch 1427/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5918 - val_loss: 17.8135\n",
      "Epoch 1428/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5933 - val_loss: 17.6709\n",
      "Epoch 1429/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5842 - val_loss: 17.6400\n",
      "Epoch 1430/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6030 - val_loss: 17.7006\n",
      "Epoch 1431/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6050 - val_loss: 17.9615\n",
      "Epoch 1432/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5976 - val_loss: 17.3825\n",
      "Epoch 1433/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5933 - val_loss: 17.9348\n",
      "Epoch 1434/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6060 - val_loss: 18.3996\n",
      "Epoch 1435/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5829 - val_loss: 18.1183\n",
      "Epoch 1436/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5951 - val_loss: 18.0242\n",
      "Epoch 1437/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6048 - val_loss: 17.9629\n",
      "Epoch 1438/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5997 - val_loss: 18.1601\n",
      "Epoch 1439/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5989 - val_loss: 19.1630\n",
      "Epoch 1440/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5929 - val_loss: 17.9993\n",
      "Epoch 1441/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6017 - val_loss: 16.5246\n",
      "Epoch 1442/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5949 - val_loss: 17.4437\n",
      "Epoch 1443/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5888 - val_loss: 17.8469\n",
      "Epoch 1444/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5950 - val_loss: 17.8153\n",
      "Epoch 1445/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6096 - val_loss: 19.0316\n",
      "Epoch 1446/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6064 - val_loss: 18.1336\n",
      "Epoch 1447/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5961 - val_loss: 17.6296\n",
      "Epoch 1448/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5887 - val_loss: 18.4843\n",
      "Epoch 1449/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5996 - val_loss: 18.7012\n",
      "Epoch 1450/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5899 - val_loss: 17.0784\n",
      "Epoch 1451/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5917 - val_loss: 17.7576\n",
      "Epoch 1452/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6036 - val_loss: 18.5586\n",
      "Epoch 1453/2000\n",
      "1552/1552 [==============================] - 52s 34ms/step - loss: 3.5796 - val_loss: 18.7861\n",
      "Epoch 1454/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.5917 - val_loss: 17.5078\n",
      "Epoch 1455/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5946 - val_loss: 17.2985\n",
      "Epoch 1456/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5989 - val_loss: 17.3541\n",
      "Epoch 1457/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5923 - val_loss: 18.2096\n",
      "Epoch 1458/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5745 - val_loss: 17.9379\n",
      "Epoch 1459/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5934 - val_loss: 17.6205\n",
      "Epoch 1460/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5941 - val_loss: 18.2652\n",
      "Epoch 1461/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5951 - val_loss: 18.5635\n",
      "Epoch 1462/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5777 - val_loss: 19.0789\n",
      "Epoch 1463/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5968 - val_loss: 17.3054\n",
      "Epoch 1464/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5908 - val_loss: 17.8842\n",
      "Epoch 1465/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5863 - val_loss: 18.3222\n",
      "Epoch 1466/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5837 - val_loss: 17.8317\n",
      "Epoch 1467/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5961 - val_loss: 18.3860\n",
      "Epoch 1468/2000\n",
      "1552/1552 [==============================] - 0s 317us/step - loss: 3.5781 - val_loss: 18.0111\n",
      "Epoch 1469/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5893 - val_loss: 18.0615\n",
      "Epoch 1470/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5840 - val_loss: 17.7287\n",
      "Epoch 1471/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5896 - val_loss: 18.4297\n",
      "Epoch 1472/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5842 - val_loss: 18.1052\n",
      "Epoch 1473/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5833 - val_loss: 18.3009\n",
      "Epoch 1474/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5995 - val_loss: 17.9922\n",
      "Epoch 1475/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5853 - val_loss: 17.1494\n",
      "Epoch 1476/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.6005 - val_loss: 17.5088\n",
      "Epoch 1477/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5916 - val_loss: 18.3953\n",
      "Epoch 1478/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5984 - val_loss: 17.6729\n",
      "Epoch 1479/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5820 - val_loss: 17.9467\n",
      "Epoch 1480/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5737 - val_loss: 17.4832\n",
      "Epoch 1481/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5933 - val_loss: 17.9751\n",
      "Epoch 1482/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5961 - val_loss: 17.6408\n",
      "Epoch 1483/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5864 - val_loss: 18.6792\n",
      "Epoch 1484/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5778 - val_loss: 17.6160\n",
      "Epoch 1485/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6015 - val_loss: 18.2824\n",
      "Epoch 1486/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5891 - val_loss: 18.6424\n",
      "Epoch 1487/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5976 - val_loss: 17.4033\n",
      "Epoch 1488/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5800 - val_loss: 17.7834\n",
      "Epoch 1489/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5832 - val_loss: 17.4907\n",
      "Epoch 1490/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5712 - val_loss: 17.2013\n",
      "Epoch 1491/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5947 - val_loss: 18.3539\n",
      "Epoch 1492/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5897 - val_loss: 18.3639\n",
      "Epoch 1493/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5745 - val_loss: 17.4665\n",
      "Epoch 1494/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5899 - val_loss: 17.7551\n",
      "Epoch 1495/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5887 - val_loss: 17.4651\n",
      "Epoch 1496/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5881 - val_loss: 18.3096\n",
      "Epoch 1497/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5780 - val_loss: 18.0359\n",
      "Epoch 1498/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6152 - val_loss: 17.3758\n",
      "Epoch 1499/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5860 - val_loss: 18.3697\n",
      "Epoch 1500/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.6032 - val_loss: 17.9693\n",
      "Epoch 1501/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5768 - val_loss: 17.9145\n",
      "Epoch 1502/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5950 - val_loss: 17.2879\n",
      "Epoch 1503/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5983 - val_loss: 17.2831\n",
      "Epoch 1504/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5926 - val_loss: 17.6039\n",
      "Epoch 1505/2000\n",
      "1552/1552 [==============================] - 0s 280us/step - loss: 3.5787 - val_loss: 17.3625\n",
      "Epoch 1506/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5975 - val_loss: 18.0932\n",
      "Epoch 1507/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5797 - val_loss: 18.3095\n",
      "Epoch 1508/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5854 - val_loss: 16.7614\n",
      "Epoch 1509/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5907 - val_loss: 17.5231\n",
      "Epoch 1510/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5837 - val_loss: 19.1105\n",
      "Epoch 1511/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5847 - val_loss: 17.7967\n",
      "Epoch 1512/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5816 - val_loss: 18.1919\n",
      "Epoch 1513/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5926 - val_loss: 17.6229\n",
      "Epoch 1514/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5763 - val_loss: 17.9283\n",
      "Epoch 1515/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5831 - val_loss: 18.1661\n",
      "Epoch 1516/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5952 - val_loss: 17.3505\n",
      "Epoch 1517/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5835 - val_loss: 17.9846\n",
      "Epoch 1518/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5782 - val_loss: 18.2588\n",
      "Epoch 1519/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5860 - val_loss: 17.3144\n",
      "Epoch 1520/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5872 - val_loss: 17.3989\n",
      "Epoch 1521/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5933 - val_loss: 18.6583\n",
      "Epoch 1522/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5914 - val_loss: 18.1974\n",
      "Epoch 1523/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5884 - val_loss: 17.6677\n",
      "Epoch 1524/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5870 - val_loss: 18.0729\n",
      "Epoch 1525/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5863 - val_loss: 18.7270\n",
      "Epoch 1526/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5878 - val_loss: 18.7044\n",
      "Epoch 1527/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5824 - val_loss: 17.4524\n",
      "Epoch 1528/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5800 - val_loss: 17.5171\n",
      "Epoch 1529/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5839 - val_loss: 18.5875\n",
      "Epoch 1530/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5895 - val_loss: 16.6343\n",
      "Epoch 1531/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5774 - val_loss: 18.0900\n",
      "Epoch 1532/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5844 - val_loss: 17.9615\n",
      "Epoch 1533/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5804 - val_loss: 17.6368\n",
      "Epoch 1534/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.6002 - val_loss: 17.0250\n",
      "Epoch 1535/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5808 - val_loss: 17.5811\n",
      "Epoch 1536/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5821 - val_loss: 17.8406\n",
      "Epoch 1537/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5845 - val_loss: 17.6371\n",
      "Epoch 1538/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5790 - val_loss: 17.9382\n",
      "Epoch 1539/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5985 - val_loss: 17.1119\n",
      "Epoch 1540/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5909 - val_loss: 18.0425\n",
      "Epoch 1541/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5785 - val_loss: 17.2784\n",
      "Epoch 1542/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5677 - val_loss: 17.0458\n",
      "Epoch 1543/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5836 - val_loss: 17.4086\n",
      "Epoch 1544/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5973 - val_loss: 18.2496\n",
      "Epoch 1545/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5797 - val_loss: 19.4343\n",
      "Epoch 1546/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5771 - val_loss: 18.1137\n",
      "Epoch 1547/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5828 - val_loss: 18.1448\n",
      "Epoch 1548/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5838 - val_loss: 18.0091\n",
      "Epoch 1549/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5780 - val_loss: 17.0411\n",
      "Epoch 1550/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5890 - val_loss: 17.3663\n",
      "Epoch 1551/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5834 - val_loss: 18.5910\n",
      "Epoch 1552/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5843 - val_loss: 17.8052\n",
      "Epoch 1553/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.6060 - val_loss: 16.9458\n",
      "Epoch 1554/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5910 - val_loss: 17.7236\n",
      "Epoch 1555/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5755 - val_loss: 17.9472\n",
      "Epoch 1556/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5632 - val_loss: 18.1250\n",
      "Epoch 1557/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5886 - val_loss: 17.6765\n",
      "Epoch 1558/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5791 - val_loss: 17.7648\n",
      "Epoch 1559/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5924 - val_loss: 18.1465\n",
      "Epoch 1560/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5767 - val_loss: 17.9535\n",
      "Epoch 1561/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5838 - val_loss: 18.1025\n",
      "Epoch 1562/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5798 - val_loss: 18.2803\n",
      "Epoch 1563/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5674 - val_loss: 18.1455\n",
      "Epoch 1564/2000\n",
      "1552/1552 [==============================] - 54s 35ms/step - loss: 3.5897 - val_loss: 17.8833\n",
      "Epoch 1565/2000\n",
      "1552/1552 [==============================] - 0s 291us/step - loss: 3.5882 - val_loss: 18.2460\n",
      "Epoch 1566/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5692 - val_loss: 17.6017\n",
      "Epoch 1567/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5805 - val_loss: 17.4969\n",
      "Epoch 1568/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5872 - val_loss: 18.0751\n",
      "Epoch 1569/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5962 - val_loss: 17.7242\n",
      "Epoch 1570/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.5793 - val_loss: 18.2411\n",
      "Epoch 1571/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5739 - val_loss: 18.4826\n",
      "Epoch 1572/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5850 - val_loss: 17.7110\n",
      "Epoch 1573/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5854 - val_loss: 17.3654\n",
      "Epoch 1574/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5727 - val_loss: 18.6516\n",
      "Epoch 1575/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6057 - val_loss: 18.3089\n",
      "Epoch 1576/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5694 - val_loss: 17.9786\n",
      "Epoch 1577/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5815 - val_loss: 17.7498\n",
      "Epoch 1578/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5877 - val_loss: 17.6040\n",
      "Epoch 1579/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5671 - val_loss: 18.4973\n",
      "Epoch 1580/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5852 - val_loss: 17.0340\n",
      "Epoch 1581/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5782 - val_loss: 17.7894\n",
      "Epoch 1582/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5738 - val_loss: 17.7846\n",
      "Epoch 1583/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5793 - val_loss: 17.7679\n",
      "Epoch 1584/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5753 - val_loss: 17.5160\n",
      "Epoch 1585/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5915 - val_loss: 17.8905\n",
      "Epoch 1586/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5844 - val_loss: 17.6443\n",
      "Epoch 1587/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5770 - val_loss: 17.9364\n",
      "Epoch 1588/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5842 - val_loss: 18.1971\n",
      "Epoch 1589/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5715 - val_loss: 17.8572\n",
      "Epoch 1590/2000\n",
      "1552/1552 [==============================] - 0s 265us/step - loss: 3.5713 - val_loss: 16.7666\n",
      "Epoch 1591/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5807 - val_loss: 17.6818\n",
      "Epoch 1592/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5880 - val_loss: 18.4926\n",
      "Epoch 1593/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5854 - val_loss: 18.3850\n",
      "Epoch 1594/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5704 - val_loss: 17.3051\n",
      "Epoch 1595/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5749 - val_loss: 17.9879\n",
      "Epoch 1596/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5886 - val_loss: 18.4825\n",
      "Epoch 1597/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5811 - val_loss: 17.1262\n",
      "Epoch 1598/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5719 - val_loss: 18.5460\n",
      "Epoch 1599/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5656 - val_loss: 17.8213\n",
      "Epoch 1600/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5986 - val_loss: 18.2853\n",
      "Epoch 1601/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5823 - val_loss: 18.5730\n",
      "Epoch 1602/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5688 - val_loss: 18.2265\n",
      "Epoch 1603/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.6013 - val_loss: 18.3383\n",
      "Epoch 1604/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5800 - val_loss: 18.1484\n",
      "Epoch 1605/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5847 - val_loss: 18.5486\n",
      "Epoch 1606/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5820 - val_loss: 18.0523\n",
      "Epoch 1607/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5997 - val_loss: 18.1476\n",
      "Epoch 1608/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5624 - val_loss: 18.5026\n",
      "Epoch 1609/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5740 - val_loss: 18.6153\n",
      "Epoch 1610/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5805 - val_loss: 17.0945\n",
      "Epoch 1611/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5755 - val_loss: 17.7524\n",
      "Epoch 1612/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5736 - val_loss: 18.7086\n",
      "Epoch 1613/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5832 - val_loss: 18.9108\n",
      "Epoch 1614/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5679 - val_loss: 18.2784\n",
      "Epoch 1615/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5742 - val_loss: 18.2942\n",
      "Epoch 1616/2000\n",
      "1552/1552 [==============================] - 0s 279us/step - loss: 3.5887 - val_loss: 18.1939\n",
      "Epoch 1617/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5915 - val_loss: 18.1748\n",
      "Epoch 1618/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5648 - val_loss: 17.4672\n",
      "Epoch 1619/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5935 - val_loss: 17.6248\n",
      "Epoch 1620/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5703 - val_loss: 17.3367\n",
      "Epoch 1621/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5739 - val_loss: 18.4196\n",
      "Epoch 1622/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5710 - val_loss: 18.2487\n",
      "Epoch 1623/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5963 - val_loss: 17.7504\n",
      "Epoch 1624/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5721 - val_loss: 17.1559\n",
      "Epoch 1625/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5603 - val_loss: 18.7064\n",
      "Epoch 1626/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5802 - val_loss: 17.6006\n",
      "Epoch 1627/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5779 - val_loss: 17.2794\n",
      "Epoch 1628/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5724 - val_loss: 18.2040\n",
      "Epoch 1629/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5833 - val_loss: 17.5356\n",
      "Epoch 1630/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5704 - val_loss: 17.5026\n",
      "Epoch 1631/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5734 - val_loss: 18.3867\n",
      "Epoch 1632/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5771 - val_loss: 17.5795\n",
      "Epoch 1633/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5831 - val_loss: 17.2355\n",
      "Epoch 1634/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5784 - val_loss: 17.0411\n",
      "Epoch 1635/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5675 - val_loss: 18.3071\n",
      "Epoch 1636/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5725 - val_loss: 17.3428\n",
      "Epoch 1637/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5686 - val_loss: 18.0732\n",
      "Epoch 1638/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5637 - val_loss: 17.1721\n",
      "Epoch 1639/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5772 - val_loss: 17.2097\n",
      "Epoch 1640/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5719 - val_loss: 17.7018\n",
      "Epoch 1641/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5730 - val_loss: 18.6552\n",
      "Epoch 1642/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5678 - val_loss: 18.5883\n",
      "Epoch 1643/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5722 - val_loss: 17.1604\n",
      "Epoch 1644/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5646 - val_loss: 18.1545\n",
      "Epoch 1645/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5884 - val_loss: 17.0791\n",
      "Epoch 1646/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5772 - val_loss: 17.4212\n",
      "Epoch 1647/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5579 - val_loss: 17.2042\n",
      "Epoch 1648/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5727 - val_loss: 17.6743\n",
      "Epoch 1649/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5692 - val_loss: 18.5173\n",
      "Epoch 1650/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5881 - val_loss: 18.1017\n",
      "Epoch 1651/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5844 - val_loss: 18.8104\n",
      "Epoch 1652/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5664 - val_loss: 17.3912\n",
      "Epoch 1653/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5699 - val_loss: 18.2817\n",
      "Epoch 1654/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5899 - val_loss: 17.5166\n",
      "Epoch 1655/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5583 - val_loss: 17.6411\n",
      "Epoch 1656/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5744 - val_loss: 17.6639\n",
      "Epoch 1657/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5744 - val_loss: 18.2190\n",
      "Epoch 1658/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5612 - val_loss: 19.1375\n",
      "Epoch 1659/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5737 - val_loss: 18.4545\n",
      "Epoch 1660/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5756 - val_loss: 18.0658\n",
      "Epoch 1661/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5790 - val_loss: 17.9041\n",
      "Epoch 1662/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5880 - val_loss: 17.5878\n",
      "Epoch 1663/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5725 - val_loss: 17.6914\n",
      "Epoch 1664/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5680 - val_loss: 17.8021\n",
      "Epoch 1665/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5630 - val_loss: 17.4031\n",
      "Epoch 1666/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5544 - val_loss: 18.5811\n",
      "Epoch 1667/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5794 - val_loss: 17.3318\n",
      "Epoch 1668/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5683 - val_loss: 19.7679\n",
      "Epoch 1669/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5795 - val_loss: 18.7851\n",
      "Epoch 1670/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5703 - val_loss: 17.0039\n",
      "Epoch 1671/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5689 - val_loss: 18.1374\n",
      "Epoch 1672/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5704 - val_loss: 17.7331\n",
      "Epoch 1673/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5736 - val_loss: 17.0160\n",
      "Epoch 1674/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5722 - val_loss: 18.2397\n",
      "Epoch 1675/2000\n",
      "1552/1552 [==============================] - 54s 35ms/step - loss: 3.5555 - val_loss: 17.3625\n",
      "Epoch 1676/2000\n",
      "1552/1552 [==============================] - 0s 283us/step - loss: 3.5818 - val_loss: 17.1705\n",
      "Epoch 1677/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5647 - val_loss: 17.7722\n",
      "Epoch 1678/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5670 - val_loss: 17.2319\n",
      "Epoch 1679/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5887 - val_loss: 17.4342\n",
      "Epoch 1680/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5626 - val_loss: 18.5585\n",
      "Epoch 1681/2000\n",
      "1552/1552 [==============================] - 0s 277us/step - loss: 3.5823 - val_loss: 17.1164\n",
      "Epoch 1682/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5867 - val_loss: 17.1862\n",
      "Epoch 1683/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5653 - val_loss: 18.6873\n",
      "Epoch 1684/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5810 - val_loss: 17.9486\n",
      "Epoch 1685/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5667 - val_loss: 18.2207\n",
      "Epoch 1686/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5613 - val_loss: 17.6244\n",
      "Epoch 1687/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5723 - val_loss: 17.2901\n",
      "Epoch 1688/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5622 - val_loss: 17.8861\n",
      "Epoch 1689/2000\n",
      "1552/1552 [==============================] - 0s 303us/step - loss: 3.5879 - val_loss: 17.4214\n",
      "Epoch 1690/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.5555 - val_loss: 17.4566\n",
      "Epoch 1691/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5640 - val_loss: 16.7657\n",
      "Epoch 1692/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5674 - val_loss: 18.3014\n",
      "Epoch 1693/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5787 - val_loss: 17.8511\n",
      "Epoch 1694/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5641 - val_loss: 17.4597\n",
      "Epoch 1695/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5800 - val_loss: 19.0942\n",
      "Epoch 1696/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5750 - val_loss: 17.3582\n",
      "Epoch 1697/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5604 - val_loss: 17.5018\n",
      "Epoch 1698/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5758 - val_loss: 17.8274\n",
      "Epoch 1699/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5595 - val_loss: 17.4518\n",
      "Epoch 1700/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5736 - val_loss: 17.5666\n",
      "Epoch 1701/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5808 - val_loss: 17.7439\n",
      "Epoch 1702/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5745 - val_loss: 17.3797\n",
      "Epoch 1703/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5594 - val_loss: 18.6711\n",
      "Epoch 1704/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5855 - val_loss: 17.6155\n",
      "Epoch 1705/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5720 - val_loss: 17.3610\n",
      "Epoch 1706/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5706 - val_loss: 17.5807\n",
      "Epoch 1707/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5531 - val_loss: 17.8922\n",
      "Epoch 1708/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5682 - val_loss: 17.9782\n",
      "Epoch 1709/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5677 - val_loss: 18.1079\n",
      "Epoch 1710/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5786 - val_loss: 18.6955\n",
      "Epoch 1711/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5618 - val_loss: 17.9052\n",
      "Epoch 1712/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5586 - val_loss: 17.4385\n",
      "Epoch 1713/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5608 - val_loss: 17.7275\n",
      "Epoch 1714/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5736 - val_loss: 18.9770\n",
      "Epoch 1715/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5819 - val_loss: 17.3545\n",
      "Epoch 1716/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5601 - val_loss: 16.6855\n",
      "Epoch 1717/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5841 - val_loss: 18.2122\n",
      "Epoch 1718/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5640 - val_loss: 17.7845\n",
      "Epoch 1719/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5850 - val_loss: 17.7778\n",
      "Epoch 1720/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5714 - val_loss: 17.8451\n",
      "Epoch 1721/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5706 - val_loss: 18.4213\n",
      "Epoch 1722/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5648 - val_loss: 17.9355\n",
      "Epoch 1723/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5654 - val_loss: 17.6627\n",
      "Epoch 1724/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5698 - val_loss: 17.9267\n",
      "Epoch 1725/2000\n",
      "1552/1552 [==============================] - 0s 282us/step - loss: 3.5729 - val_loss: 17.4432\n",
      "Epoch 1726/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5729 - val_loss: 18.3857\n",
      "Epoch 1727/2000\n",
      "1552/1552 [==============================] - 0s 290us/step - loss: 3.5524 - val_loss: 17.6062\n",
      "Epoch 1728/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5664 - val_loss: 18.2898\n",
      "Epoch 1729/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5609 - val_loss: 17.3832\n",
      "Epoch 1730/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5563 - val_loss: 18.2748\n",
      "Epoch 1731/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5828 - val_loss: 17.7656\n",
      "Epoch 1732/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5664 - val_loss: 17.7010\n",
      "Epoch 1733/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5669 - val_loss: 17.5415\n",
      "Epoch 1734/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5664 - val_loss: 17.4929\n",
      "Epoch 1735/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5782 - val_loss: 17.7801\n",
      "Epoch 1736/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5540 - val_loss: 18.6584\n",
      "Epoch 1737/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5637 - val_loss: 17.1311\n",
      "Epoch 1738/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5660 - val_loss: 17.6827\n",
      "Epoch 1739/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5720 - val_loss: 17.7524\n",
      "Epoch 1740/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5628 - val_loss: 17.7462\n",
      "Epoch 1741/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5637 - val_loss: 17.4524\n",
      "Epoch 1742/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5631 - val_loss: 17.5971\n",
      "Epoch 1743/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5726 - val_loss: 17.2958\n",
      "Epoch 1744/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5572 - val_loss: 17.2467\n",
      "Epoch 1745/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5598 - val_loss: 17.3369\n",
      "Epoch 1746/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5772 - val_loss: 17.7781\n",
      "Epoch 1747/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5573 - val_loss: 17.6975\n",
      "Epoch 1748/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5476 - val_loss: 18.6673\n",
      "Epoch 1749/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5687 - val_loss: 17.0439\n",
      "Epoch 1750/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5685 - val_loss: 17.8574\n",
      "Epoch 1751/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5689 - val_loss: 17.6490\n",
      "Epoch 1752/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5642 - val_loss: 17.9746\n",
      "Epoch 1753/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5702 - val_loss: 16.4651\n",
      "Epoch 1754/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5597 - val_loss: 17.9212\n",
      "Epoch 1755/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5596 - val_loss: 17.5735\n",
      "Epoch 1756/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5635 - val_loss: 17.3832\n",
      "Epoch 1757/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5704 - val_loss: 18.0140\n",
      "Epoch 1758/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5624 - val_loss: 17.6381\n",
      "Epoch 1759/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5479 - val_loss: 18.4480\n",
      "Epoch 1760/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5564 - val_loss: 18.4018\n",
      "Epoch 1761/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5568 - val_loss: 18.3199\n",
      "Epoch 1762/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5667 - val_loss: 17.0710\n",
      "Epoch 1763/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5612 - val_loss: 17.7911\n",
      "Epoch 1764/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5588 - val_loss: 18.5314\n",
      "Epoch 1765/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5696 - val_loss: 18.0364\n",
      "Epoch 1766/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5713 - val_loss: 18.0533\n",
      "Epoch 1767/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5690 - val_loss: 17.7496\n",
      "Epoch 1768/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5580 - val_loss: 16.8672\n",
      "Epoch 1769/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5625 - val_loss: 17.6154\n",
      "Epoch 1770/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5589 - val_loss: 17.6458\n",
      "Epoch 1771/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5616 - val_loss: 17.4951\n",
      "Epoch 1772/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5650 - val_loss: 17.4820\n",
      "Epoch 1773/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5811 - val_loss: 17.4716\n",
      "Epoch 1774/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5470 - val_loss: 17.8577\n",
      "Epoch 1775/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5608 - val_loss: 17.6605\n",
      "Epoch 1776/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5526 - val_loss: 18.1736\n",
      "Epoch 1777/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5600 - val_loss: 18.0219\n",
      "Epoch 1778/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5712 - val_loss: 17.7274\n",
      "Epoch 1779/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5673 - val_loss: 17.5633\n",
      "Epoch 1780/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5672 - val_loss: 17.9480\n",
      "Epoch 1781/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5667 - val_loss: 18.2267\n",
      "Epoch 1782/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5488 - val_loss: 17.6668\n",
      "Epoch 1783/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5565 - val_loss: 18.0299\n",
      "Epoch 1784/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5538 - val_loss: 18.6522\n",
      "Epoch 1785/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5758 - val_loss: 18.2825\n",
      "Epoch 1786/2000\n",
      "1552/1552 [==============================] - 56s 36ms/step - loss: 3.5579 - val_loss: 17.5686\n",
      "Epoch 1787/2000\n",
      "1552/1552 [==============================] - 0s 294us/step - loss: 3.5720 - val_loss: 18.3595\n",
      "Epoch 1788/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5530 - val_loss: 17.0610\n",
      "Epoch 1789/2000\n",
      "1552/1552 [==============================] - 0s 278us/step - loss: 3.5535 - val_loss: 18.0141\n",
      "Epoch 1790/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5602 - val_loss: 17.5951\n",
      "Epoch 1791/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5564 - val_loss: 16.9519\n",
      "Epoch 1792/2000\n",
      "1552/1552 [==============================] - 1s 323us/step - loss: 3.5677 - val_loss: 17.9209\n",
      "Epoch 1793/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5589 - val_loss: 18.1763\n",
      "Epoch 1794/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5599 - val_loss: 17.8676\n",
      "Epoch 1795/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5800 - val_loss: 18.3428\n",
      "Epoch 1796/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5681 - val_loss: 17.4092\n",
      "Epoch 1797/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5505 - val_loss: 17.7493\n",
      "Epoch 1798/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5550 - val_loss: 17.7329\n",
      "Epoch 1799/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5720 - val_loss: 17.5952\n",
      "Epoch 1800/2000\n",
      "1552/1552 [==============================] - 1s 323us/step - loss: 3.5556 - val_loss: 17.9920\n",
      "Epoch 1801/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5560 - val_loss: 17.4188\n",
      "Epoch 1802/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5516 - val_loss: 18.2643\n",
      "Epoch 1803/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5648 - val_loss: 17.5095\n",
      "Epoch 1804/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5462 - val_loss: 17.4412\n",
      "Epoch 1805/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5627 - val_loss: 17.8131\n",
      "Epoch 1806/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5478 - val_loss: 16.9062\n",
      "Epoch 1807/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5569 - val_loss: 17.0957\n",
      "Epoch 1808/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5647 - val_loss: 18.2195\n",
      "Epoch 1809/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5704 - val_loss: 17.6270\n",
      "Epoch 1810/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5622 - val_loss: 17.6885\n",
      "Epoch 1811/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5583 - val_loss: 18.1491\n",
      "Epoch 1812/2000\n",
      "1552/1552 [==============================] - 0s 268us/step - loss: 3.5648 - val_loss: 18.1152\n",
      "Epoch 1813/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5562 - val_loss: 17.6994\n",
      "Epoch 1814/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5495 - val_loss: 17.2849\n",
      "Epoch 1815/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5621 - val_loss: 18.8154\n",
      "Epoch 1816/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5732 - val_loss: 17.5262\n",
      "Epoch 1817/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5497 - val_loss: 17.9213\n",
      "Epoch 1818/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5553 - val_loss: 18.0370\n",
      "Epoch 1819/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5635 - val_loss: 16.5247\n",
      "Epoch 1820/2000\n",
      "1552/1552 [==============================] - 1s 698us/step - loss: 3.5583 - val_loss: 17.5635\n",
      "Epoch 1821/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5622 - val_loss: 17.0792\n",
      "Epoch 1822/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5500 - val_loss: 17.9402\n",
      "Epoch 1823/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5703 - val_loss: 17.2149\n",
      "Epoch 1824/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5636 - val_loss: 17.6676\n",
      "Epoch 1825/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5505 - val_loss: 17.3350\n",
      "Epoch 1826/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5441 - val_loss: 17.9140\n",
      "Epoch 1827/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5549 - val_loss: 16.8810\n",
      "Epoch 1828/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5675 - val_loss: 17.6613\n",
      "Epoch 1829/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5440 - val_loss: 17.7992\n",
      "Epoch 1830/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5564 - val_loss: 18.2400\n",
      "Epoch 1831/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5570 - val_loss: 18.2471\n",
      "Epoch 1832/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5668 - val_loss: 18.0170\n",
      "Epoch 1833/2000\n",
      "1552/1552 [==============================] - 0s 285us/step - loss: 3.5430 - val_loss: 17.9121\n",
      "Epoch 1834/2000\n",
      "1552/1552 [==============================] - 0s 283us/step - loss: 3.5749 - val_loss: 17.3711\n",
      "Epoch 1835/2000\n",
      "1552/1552 [==============================] - 0s 281us/step - loss: 3.5472 - val_loss: 18.2562\n",
      "Epoch 1836/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5535 - val_loss: 17.3695\n",
      "Epoch 1837/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5627 - val_loss: 18.1156\n",
      "Epoch 1838/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5547 - val_loss: 17.8865\n",
      "Epoch 1839/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5446 - val_loss: 18.2473\n",
      "Epoch 1840/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5534 - val_loss: 17.4502\n",
      "Epoch 1841/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5562 - val_loss: 17.8156\n",
      "Epoch 1842/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5502 - val_loss: 17.4512\n",
      "Epoch 1843/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5635 - val_loss: 18.0707\n",
      "Epoch 1844/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5399 - val_loss: 17.7006\n",
      "Epoch 1845/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5595 - val_loss: 17.9851\n",
      "Epoch 1846/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5586 - val_loss: 17.6337\n",
      "Epoch 1847/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5406 - val_loss: 17.0227\n",
      "Epoch 1848/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5561 - val_loss: 16.8319\n",
      "Epoch 1849/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5860 - val_loss: 18.1133\n",
      "Epoch 1850/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5409 - val_loss: 16.7105\n",
      "Epoch 1851/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5539 - val_loss: 17.3565\n",
      "Epoch 1852/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5628 - val_loss: 18.2039\n",
      "Epoch 1853/2000\n",
      "1552/1552 [==============================] - 0s 269us/step - loss: 3.5331 - val_loss: 18.4240\n",
      "Epoch 1854/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5675 - val_loss: 17.8555\n",
      "Epoch 1855/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5433 - val_loss: 18.0499\n",
      "Epoch 1856/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5512 - val_loss: 18.4507\n",
      "Epoch 1857/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5630 - val_loss: 18.2276\n",
      "Epoch 1858/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5564 - val_loss: 18.3452\n",
      "Epoch 1859/2000\n",
      "1552/1552 [==============================] - 0s 318us/step - loss: 3.5582 - val_loss: 18.0265\n",
      "Epoch 1860/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5608 - val_loss: 19.0168\n",
      "Epoch 1861/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5627 - val_loss: 17.5346\n",
      "Epoch 1862/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5560 - val_loss: 17.3874\n",
      "Epoch 1863/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5583 - val_loss: 16.9590\n",
      "Epoch 1864/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5575 - val_loss: 18.0449\n",
      "Epoch 1865/2000\n",
      "1552/1552 [==============================] - 0s 276us/step - loss: 3.5475 - val_loss: 17.4317\n",
      "Epoch 1866/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5513 - val_loss: 17.4365\n",
      "Epoch 1867/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5607 - val_loss: 17.0932\n",
      "Epoch 1868/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5818 - val_loss: 17.3657\n",
      "Epoch 1869/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5510 - val_loss: 18.0796\n",
      "Epoch 1870/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5860 - val_loss: 17.5026\n",
      "Epoch 1871/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5496 - val_loss: 17.3675\n",
      "Epoch 1872/2000\n",
      "1552/1552 [==============================] - 0s 275us/step - loss: 3.5400 - val_loss: 17.9134\n",
      "Epoch 1873/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5539 - val_loss: 18.1179\n",
      "Epoch 1874/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5682 - val_loss: 17.7208\n",
      "Epoch 1875/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5573 - val_loss: 17.2802\n",
      "Epoch 1876/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5630 - val_loss: 17.3983\n",
      "Epoch 1877/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5567 - val_loss: 17.8285\n",
      "Epoch 1878/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5479 - val_loss: 17.7089\n",
      "Epoch 1879/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5632 - val_loss: 17.6956\n",
      "Epoch 1880/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5516 - val_loss: 17.9686\n",
      "Epoch 1881/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5605 - val_loss: 18.1584\n",
      "Epoch 1882/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5639 - val_loss: 17.8276\n",
      "Epoch 1883/2000\n",
      "1552/1552 [==============================] - 0s 270us/step - loss: 3.5644 - val_loss: 17.5988\n",
      "Epoch 1884/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5470 - val_loss: 17.3126\n",
      "Epoch 1885/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5577 - val_loss: 17.5803\n",
      "Epoch 1886/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5466 - val_loss: 18.0603\n",
      "Epoch 1887/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5418 - val_loss: 16.9714\n",
      "Epoch 1888/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5647 - val_loss: 18.0269\n",
      "Epoch 1889/2000\n",
      "1552/1552 [==============================] - 0s 272us/step - loss: 3.5514 - val_loss: 18.0971\n",
      "Epoch 1890/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5524 - val_loss: 18.5648\n",
      "Epoch 1891/2000\n",
      "1552/1552 [==============================] - 0s 274us/step - loss: 3.5487 - val_loss: 16.8487\n",
      "Epoch 1892/2000\n",
      "1552/1552 [==============================] - 0s 273us/step - loss: 3.5600 - val_loss: 18.1097\n",
      "Epoch 1893/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5511 - val_loss: 19.3625\n",
      "Epoch 1894/2000\n",
      "1552/1552 [==============================] - 0s 271us/step - loss: 3.5589 - val_loss: 17.3077\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train.reshape((-1, 1)), epochs=2000, batch_size=None, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07cd0874-3a2e-4ea3-8eeb-bdc8124f24d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388/388 [==============================] - 0s 169us/step\n",
      "13.238028350384157 0.6457969516630517\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(mean_squared_error(Y_test, preds), r2_score(Y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e81c7083-e9df-4c6e-826d-0e2b4ee190cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Train Random Forest model\n",
    "rfr = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rfr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0377119-f446-459e-b6aa-7f2e44b02a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.690683845885678 0.7139549254017582\n"
     ]
    }
   ],
   "source": [
    "preds = rfr.predict(X_test)\n",
    "print(mean_squared_error(Y_test, preds), r2_score(Y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9966697-abb5-4085-90e8-46f49c3183fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c3523-172a-45f9-ae9a-a8c255d5c42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5701e22-e700-4617-b5ed-95afde35d340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77777c24-ca34-4f81-81d2-1b967d38663b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53abd387-ab6f-4e30-92a9-d4e18baafa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 4992/4992 [00:01<00:00, 2682.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data, gap_type):\n",
    "\n",
    "    time_deltas = {\n",
    "        \"very short\": pd.Timedelta(minutes=30),\n",
    "        \"short\": pd.Timedelta(hours=4),\n",
    "        \"medium\": pd.Timedelta(hours=32),\n",
    "        \"week\": pd.Timedelta(days=7),\n",
    "        \"long\": pd.Timedelta(days=12),\n",
    "        \"month\": pd.Timedelta(days=30)\n",
    "    }\n",
    "    \n",
    "    gap_length = time_deltas[gap_type]\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data[:5000]))):\n",
    "        seq_end_time = data['DateTime'].iloc[i + sequence_length - 1]\n",
    "        target_end_time = seq_end_time + gap_length\n",
    "        \n",
    "        if target_end_time > data['DateTime'].iloc[-1]:\n",
    "            break\n",
    "        \n",
    "        seq = data.iloc[i:i + sequence_length]\n",
    "        target = data.loc[(data['DateTime'] > seq_end_time) & (data['DateTime'] <= target_end_time), 'NEE'].values\n",
    "        \n",
    "        if len(target) > 0 and not np.isnan(target).any():\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    X = np.array([seq[columns_to_pick].values for seq in sequences])\n",
    "    y = targets  # Keeping the targets as a list to handle dynamic lengths\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Example Usage for Version 2\n",
    "X, Y = prepare_lstm_data_version2(data, sequence_type=\"short\", gap_type=\"short\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5c895b-817d-4762-a3af-72f8f27e39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lstm_data.pickle\", \"wb\") as fp:\n",
    "    pickle.dump({\"X\": X, \"Y\": Y}, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
